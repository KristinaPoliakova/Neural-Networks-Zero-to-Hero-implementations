{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "\n",
    "- Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\n",
    "- Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\n",
    "- Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "splits = random_split(words, [0.8, 0.1, 0.1], generator=generator)\n",
    "train_data, val_data, test_data = splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation data\n",
    "val_xs, val_ys = [], []\n",
    "for w in val_data:\n",
    "    chs = ['.','.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        combined_ix = ix1 * 27 + ix2\n",
    "        val_xs.append(combined_ix)\n",
    "        val_ys.append(stoi[ch3])\n",
    "\n",
    "val_xs = torch.tensor(val_xs)\n",
    "val_ys = torch.tensor(val_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data (same way we created validation data)\n",
    "test_xs, test_ys = [], []\n",
    "for w in test_data:   \n",
    "    chs = ['.','.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        combined_ix = ix1 * 27 + ix2\n",
    "        test_xs.append(combined_ix)\n",
    "        test_ys.append(stoi[ch3])\n",
    "\n",
    "test_xs = torch.tensor(test_xs)\n",
    "test_ys = torch.tensor(test_ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(train_data))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "print(stoi)\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  182827\n",
      "xs shape:  torch.Size([182827])\n",
      "ys shape:  torch.Size([182827])\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in train_data:\n",
    "    chs = ['.', '.'] + list(w) + ['.'] # add the double padding at start \n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "\n",
    "        combined_ix = ix1 * 27 + ix2\n",
    "        \n",
    "        # encode 2 letters into one unique number with no collisions\n",
    "        #        Second Letter →\n",
    "        #       .  a  b  c ...\n",
    "        #F  .   0  1  2  3 ...\n",
    "        #i  a  27 28 29 30 ...\n",
    "        #r  b  54 55 56 57 ...\n",
    "        #s  c  81 82 83 84 ...\n",
    "        #t  .   .  .  .  . ...\n",
    "        #↓\n",
    "        \n",
    "        xs.append(combined_ix)\n",
    "        ys.append(ix3)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "print('xs shape: ', xs.shape) \n",
    "print('ys shape: ', ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.748319149017334\n",
      "step 0: train loss 3.7483, val loss 3.7543\n",
      "3.6344263553619385\n",
      "3.550145387649536\n",
      "3.484727382659912\n",
      "3.429079294204712\n",
      "3.3790476322174072\n",
      "3.3334155082702637\n",
      "3.2915987968444824\n",
      "3.253203868865967\n",
      "3.2179064750671387\n",
      "3.185405731201172\n",
      "step 10: train loss 3.1854, val loss 3.1927\n",
      "3.155413866043091\n",
      "3.1276533603668213\n",
      "3.1018667221069336\n",
      "3.0778205394744873\n",
      "3.0553109645843506\n",
      "3.034165382385254\n",
      "3.014239549636841\n",
      "2.9954140186309814\n",
      "2.9775891304016113\n",
      "2.960679054260254\n",
      "step 20: train loss 2.9607, val loss 2.9687\n",
      "2.944612503051758\n",
      "2.929326057434082\n",
      "2.9147629737854004\n",
      "2.9008727073669434\n",
      "2.8876099586486816\n",
      "2.8749325275421143\n",
      "2.862802743911743\n",
      "2.851184844970703\n",
      "2.8400468826293945\n",
      "2.8293585777282715\n",
      "step 30: train loss 2.8294, val loss 2.8381\n",
      "2.819092273712158\n",
      "2.8092219829559326\n",
      "2.7997238636016846\n",
      "2.7905750274658203\n",
      "2.781755208969116\n",
      "2.7732458114624023\n",
      "2.7650275230407715\n",
      "2.757084846496582\n",
      "2.7494020462036133\n",
      "2.741964340209961\n",
      "step 40: train loss 2.7420, val loss 2.7521\n",
      "2.7347593307495117\n",
      "2.7277746200561523\n",
      "2.7209982872009277\n",
      "2.7144196033477783\n",
      "2.708029270172119\n",
      "2.7018179893493652\n",
      "2.6957767009735107\n",
      "2.6898982524871826\n",
      "2.6841742992401123\n",
      "2.678598642349243\n",
      "step 50: train loss 2.6786, val loss 2.6902\n",
      "2.6731646060943604\n",
      "2.6678664684295654\n",
      "2.6626980304718018\n",
      "2.65765380859375\n",
      "2.6527297496795654\n",
      "2.6479198932647705\n",
      "2.643221139907837\n",
      "2.6386282444000244\n",
      "2.6341378688812256\n",
      "2.6297459602355957\n",
      "step 60: train loss 2.6297, val loss 2.6427\n",
      "2.6254496574401855\n",
      "2.6212451457977295\n",
      "2.61712908744812\n",
      "2.6130993366241455\n",
      "2.609152317047119\n",
      "2.605285882949829\n",
      "2.601496934890747\n",
      "2.597783327102661\n",
      "2.5941429138183594\n",
      "2.5905730724334717\n",
      "step 70: train loss 2.5906, val loss 2.6046\n",
      "2.5870726108551025\n",
      "2.5836384296417236\n",
      "2.5802688598632812\n",
      "2.576962471008301\n",
      "2.5737175941467285\n",
      "2.5705316066741943\n",
      "2.567403554916382\n",
      "2.5643320083618164\n",
      "2.5613152980804443\n",
      "2.5583510398864746\n",
      "step 80: train loss 2.5584, val loss 2.5733\n",
      "2.5554392337799072\n",
      "2.5525779724121094\n",
      "2.5497658252716064\n",
      "2.547001361846924\n",
      "2.544283390045166\n",
      "2.541611433029175\n",
      "2.5389838218688965\n",
      "2.5363991260528564\n",
      "2.5338566303253174\n",
      "2.5313549041748047\n",
      "step 90: train loss 2.5314, val loss 2.5471\n",
      "2.52889347076416\n",
      "2.526470899581909\n",
      "2.5240867137908936\n",
      "2.5217397212982178\n",
      "2.5194287300109863\n",
      "2.517153263092041\n",
      "2.5149123668670654\n",
      "2.5127053260803223\n",
      "2.510530948638916\n",
      "2.5083887577056885\n",
      "step 100: train loss 2.5084, val loss 2.5249\n",
      "2.5062780380249023\n",
      "2.5041980743408203\n",
      "2.502147912979126\n",
      "2.5001273155212402\n",
      "2.4981348514556885\n",
      "2.49617075920105\n",
      "2.4942336082458496\n",
      "2.492323637008667\n",
      "2.4904396533966064\n",
      "2.4885809421539307\n",
      "step 110: train loss 2.4886, val loss 2.5057\n",
      "2.4867477416992188\n",
      "2.484938621520996\n",
      "2.483153820037842\n",
      "2.4813923835754395\n",
      "2.4796535968780518\n",
      "2.477937698364258\n",
      "2.476243019104004\n",
      "2.4745705127716064\n",
      "2.472919225692749\n",
      "2.4712882041931152\n",
      "step 120: train loss 2.4713, val loss 2.4891\n",
      "2.469677448272705\n",
      "2.4680871963500977\n",
      "2.46651554107666\n",
      "2.464963674545288\n",
      "2.4634299278259277\n",
      "2.4619152545928955\n",
      "2.460418462753296\n",
      "2.4589388370513916\n",
      "2.45747709274292\n",
      "2.4560320377349854\n",
      "step 130: train loss 2.4560, val loss 2.4744\n",
      "2.454603910446167\n",
      "2.4531919956207275\n",
      "2.451796531677246\n",
      "2.4504165649414062\n",
      "2.449052572250366\n",
      "2.4477038383483887\n",
      "2.4463696479797363\n",
      "2.4450504779815674\n",
      "2.4437460899353027\n",
      "2.442455768585205\n",
      "step 140: train loss 2.4425, val loss 2.4613\n",
      "2.441179037094116\n",
      "2.4399168491363525\n",
      "2.4386675357818604\n",
      "2.437432050704956\n",
      "2.4362096786499023\n",
      "2.434999942779541\n",
      "2.433803081512451\n",
      "2.4326186180114746\n",
      "2.4314465522766113\n",
      "2.430286407470703\n",
      "step 150: train loss 2.4303, val loss 2.4497\n",
      "2.429138660430908\n",
      "2.42800235748291\n",
      "2.42687726020813\n",
      "2.4257640838623047\n",
      "2.424661636352539\n",
      "2.4235706329345703\n",
      "2.422490119934082\n",
      "2.4214208126068115\n",
      "2.4203615188598633\n",
      "2.4193129539489746\n",
      "step 160: train loss 2.4193, val loss 2.4392\n",
      "2.4182746410369873\n",
      "2.417246103286743\n",
      "2.4162278175354004\n",
      "2.415219306945801\n",
      "2.4142203330993652\n",
      "2.413231134414673\n",
      "2.4122509956359863\n",
      "2.411280393600464\n",
      "2.4103188514709473\n",
      "2.4093658924102783\n",
      "step 170: train loss 2.4094, val loss 2.4297\n",
      "2.4084219932556152\n",
      "2.407486915588379\n",
      "2.4065608978271484\n",
      "2.4056429862976074\n",
      "2.404733180999756\n",
      "2.403832197189331\n",
      "2.4029390811920166\n",
      "2.4020540714263916\n",
      "2.401176929473877\n",
      "2.4003076553344727\n",
      "step 180: train loss 2.4003, val loss 2.4211\n",
      "2.3994462490081787\n",
      "2.398592233657837\n",
      "2.3977463245391846\n",
      "2.396907329559326\n",
      "2.39607572555542\n",
      "2.395251750946045\n",
      "2.394434690475464\n",
      "2.3936245441436768\n",
      "2.3928213119506836\n",
      "2.3920249938964844\n",
      "step 190: train loss 2.3920, val loss 2.4132\n",
      "2.391235589981079\n",
      "2.3904528617858887\n",
      "2.389677047729492\n",
      "2.3889071941375732\n",
      "2.388144016265869\n",
      "2.38738751411438\n",
      "2.3866372108459473\n",
      "2.385892629623413\n",
      "2.3851547241210938\n",
      "2.384422540664673\n",
      "step 200: train loss 2.3844, val loss 2.4060\n",
      "2.3836965560913086\n",
      "2.382976531982422\n",
      "2.3822622299194336\n",
      "2.3815536499023438\n",
      "2.3808507919311523\n",
      "2.3801538944244385\n",
      "2.379462242126465\n",
      "2.3787760734558105\n",
      "2.3780951499938965\n",
      "2.37742018699646\n",
      "step 210: train loss 2.3774, val loss 2.3993\n",
      "2.3767499923706055\n",
      "2.3760855197906494\n",
      "2.3754255771636963\n",
      "2.3747713565826416\n",
      "2.374121904373169\n",
      "2.3734774589538574\n",
      "2.372838020324707\n",
      "2.3722035884857178\n",
      "2.3715741634368896\n",
      "2.3709487915039062\n",
      "step 220: train loss 2.3709, val loss 2.3932\n",
      "2.370328903198242\n",
      "2.369713306427002\n",
      "2.3691024780273438\n",
      "2.3684959411621094\n",
      "2.367893934249878\n",
      "2.3672964572906494\n",
      "2.366703748703003\n",
      "2.366114616394043\n",
      "2.365530490875244\n",
      "2.364950180053711\n",
      "step 230: train loss 2.3650, val loss 2.3875\n",
      "2.3643743991851807\n",
      "2.363802671432495\n",
      "2.3632349967956543\n",
      "2.3626718521118164\n",
      "2.362112045288086\n",
      "2.3615562915802\n",
      "2.3610050678253174\n",
      "2.360457181930542\n",
      "2.3599138259887695\n",
      "2.3593735694885254\n",
      "step 240: train loss 2.3594, val loss 2.3823\n",
      "2.358837604522705\n",
      "2.358304977416992\n",
      "2.357776403427124\n",
      "2.3572511672973633\n",
      "2.356729507446289\n",
      "2.3562116622924805\n",
      "2.3556973934173584\n",
      "2.3551864624023438\n",
      "2.3546793460845947\n",
      "2.354175090789795\n",
      "step 250: train loss 2.3542, val loss 2.3774\n",
      "2.3536744117736816\n",
      "2.353177547454834\n",
      "2.3526835441589355\n",
      "2.3521928787231445\n",
      "2.351705551147461\n",
      "2.3512213230133057\n",
      "2.350740671157837\n",
      "2.3502628803253174\n",
      "2.349788188934326\n",
      "2.3493165969848633\n",
      "step 260: train loss 2.3493, val loss 2.3728\n",
      "2.348848342895508\n",
      "2.3483827114105225\n",
      "2.3479204177856445\n",
      "2.347460985183716\n",
      "2.3470048904418945\n",
      "2.346550703048706\n",
      "2.3461005687713623\n",
      "2.3456525802612305\n",
      "2.345207452774048\n",
      "2.3447651863098145\n",
      "step 270: train loss 2.3448, val loss 2.3685\n",
      "2.3443257808685303\n",
      "2.3438894748687744\n",
      "2.3434555530548096\n",
      "2.3430237770080566\n",
      "2.3425955772399902\n",
      "2.3421695232391357\n",
      "2.3417465686798096\n",
      "2.3413257598876953\n",
      "2.340907573699951\n",
      "2.3404922485351562\n",
      "step 280: train loss 2.3405, val loss 2.3645\n",
      "2.3400790691375732\n",
      "2.3396685123443604\n",
      "2.3392605781555176\n",
      "2.338855028152466\n",
      "2.338451862335205\n",
      "2.3380510807037354\n",
      "2.3376526832580566\n",
      "2.337257146835327\n",
      "2.3368632793426514\n",
      "2.3364717960357666\n",
      "step 290: train loss 2.3365, val loss 2.3607\n",
      "2.336082935333252\n",
      "2.33569598197937\n",
      "2.3353116512298584\n",
      "2.3349294662475586\n",
      "2.334549903869629\n",
      "2.334171772003174\n",
      "2.3337960243225098\n",
      "2.3334226608276367\n",
      "2.3330512046813965\n",
      "2.3326823711395264\n",
      "step 300: train loss 2.3327, val loss 2.3572\n",
      "2.332314968109131\n",
      "2.3319504261016846\n",
      "2.331587314605713\n",
      "2.3312265872955322\n",
      "2.3308680057525635\n",
      "2.3305110931396484\n",
      "2.3301565647125244\n",
      "2.329803466796875\n",
      "2.3294527530670166\n",
      "2.329103946685791\n",
      "step 310: train loss 2.3291, val loss 2.3538\n",
      "2.3287570476531982\n",
      "2.3284120559692383\n",
      "2.328068733215332\n",
      "2.327727794647217\n",
      "2.3273885250091553\n",
      "2.3270506858825684\n",
      "2.3267149925231934\n",
      "2.3263814449310303\n",
      "2.326049327850342\n",
      "2.325718879699707\n",
      "step 320: train loss 2.3257, val loss 2.3507\n",
      "2.3253908157348633\n",
      "2.325063943862915\n",
      "2.3247389793395996\n",
      "2.324415922164917\n",
      "2.324094295501709\n",
      "2.323774814605713\n",
      "2.3234565258026123\n",
      "2.3231403827667236\n",
      "2.3228254318237305\n",
      "2.322512626647949\n",
      "step 330: train loss 2.3225, val loss 2.3477\n",
      "2.3222012519836426\n",
      "2.3218917846679688\n",
      "2.3215832710266113\n",
      "2.321276903152466\n",
      "2.320971727371216\n",
      "2.3206684589385986\n",
      "2.320366859436035\n",
      "2.320066452026367\n",
      "2.319767475128174\n",
      "2.3194706439971924\n",
      "step 340: train loss 2.3195, val loss 2.3449\n",
      "2.3191750049591064\n",
      "2.318881034851074\n",
      "2.3185882568359375\n",
      "2.3182971477508545\n",
      "2.318007230758667\n",
      "2.317718744277954\n",
      "2.317432403564453\n",
      "2.3171470165252686\n",
      "2.3168630599975586\n",
      "2.3165805339813232\n",
      "step 350: train loss 2.3166, val loss 2.3422\n",
      "2.3162994384765625\n",
      "2.3160197734832764\n",
      "2.3157413005828857\n",
      "2.315464496612549\n",
      "2.3151888847351074\n",
      "2.3149147033691406\n",
      "2.3146421909332275\n",
      "2.3143701553344727\n",
      "2.314100503921509\n",
      "2.313831090927124\n",
      "step 360: train loss 2.3138, val loss 2.3396\n",
      "2.313563823699951\n",
      "2.3132975101470947\n",
      "2.313032627105713\n",
      "2.3127686977386475\n",
      "2.3125061988830566\n",
      "2.3122448921203613\n",
      "2.311985492706299\n",
      "2.3117268085479736\n",
      "2.311469078063965\n",
      "2.3112130165100098\n",
      "step 370: train loss 2.3112, val loss 2.3372\n",
      "2.310957670211792\n",
      "2.310703992843628\n",
      "2.3104512691497803\n",
      "2.3101999759674072\n",
      "2.3099498748779297\n",
      "2.3097007274627686\n",
      "2.309452533721924\n",
      "2.309206008911133\n",
      "2.308960199356079\n",
      "2.3087158203125\n",
      "step 380: train loss 2.3087, val loss 2.3348\n",
      "2.3084723949432373\n",
      "2.308230400085449\n",
      "2.3079893589019775\n",
      "2.3077495098114014\n",
      "2.3075106143951416\n",
      "2.3072726726531982\n",
      "2.3070361614227295\n",
      "2.306800603866577\n",
      "2.306565761566162\n",
      "2.3063321113586426\n",
      "step 390: train loss 2.3063, val loss 2.3326\n",
      "2.3060998916625977\n",
      "2.30586838722229\n",
      "2.305638074874878\n",
      "2.3054089546203613\n",
      "2.305180549621582\n",
      "2.3049533367156982\n",
      "2.304727077484131\n",
      "2.30450177192688\n",
      "2.3042776584625244\n",
      "2.3040544986724854\n",
      "step 400: train loss 2.3041, val loss 2.3305\n",
      "2.3038320541381836\n",
      "2.3036108016967773\n",
      "2.3033907413482666\n",
      "2.303171396255493\n",
      "2.302952527999878\n",
      "2.3027353286743164\n",
      "2.3025190830230713\n",
      "2.3023033142089844\n",
      "2.302088975906372\n",
      "2.301875114440918\n",
      "step 410: train loss 2.3019, val loss 2.3285\n",
      "2.3016622066497803\n",
      "2.301450490951538\n",
      "2.3012397289276123\n",
      "2.301029682159424\n",
      "2.300820827484131\n",
      "2.300612688064575\n",
      "2.300405263900757\n",
      "2.300198793411255\n",
      "2.2999930381774902\n",
      "2.299788475036621\n",
      "step 420: train loss 2.2998, val loss 2.3266\n",
      "2.2995846271514893\n",
      "2.2993812561035156\n",
      "2.2991795539855957\n",
      "2.298978328704834\n",
      "2.2987778186798096\n",
      "2.2985782623291016\n",
      "2.298379421234131\n",
      "2.2981815338134766\n",
      "2.2979843616485596\n",
      "2.297788143157959\n",
      "step 430: train loss 2.2978, val loss 2.3247\n",
      "2.2975926399230957\n",
      "2.2973978519439697\n",
      "2.2972044944763184\n",
      "2.297011137008667\n",
      "2.296818733215332\n",
      "2.2966272830963135\n",
      "2.2964365482330322\n",
      "2.2962465286254883\n",
      "2.2960574626922607\n",
      "2.2958691120147705\n",
      "step 440: train loss 2.2959, val loss 2.3230\n",
      "2.2956814765930176\n",
      "2.295494556427002\n",
      "2.2953083515167236\n",
      "2.2951231002807617\n",
      "2.294938325881958\n",
      "2.29475474357605\n",
      "2.2945716381073\n",
      "2.294389009475708\n",
      "2.2942073345184326\n",
      "2.2940263748168945\n",
      "step 450: train loss 2.2940, val loss 2.3213\n",
      "2.293846368789673\n",
      "2.2936668395996094\n",
      "2.293487787246704\n",
      "2.2933096885681152\n",
      "2.2931320667266846\n",
      "2.2929553985595703\n",
      "2.2927796840667725\n",
      "2.292604446411133\n",
      "2.2924294471740723\n",
      "2.2922556400299072\n",
      "step 460: train loss 2.2923, val loss 2.3196\n",
      "2.2920823097229004\n",
      "2.2919094562530518\n",
      "2.2917380332946777\n",
      "2.2915663719177246\n",
      "2.291395664215088\n",
      "2.2912256717681885\n",
      "2.2910563945770264\n",
      "2.2908878326416016\n",
      "2.290719747543335\n",
      "2.2905526161193848\n",
      "step 470: train loss 2.2906, val loss 2.3181\n",
      "2.2903854846954346\n",
      "2.29021954536438\n",
      "2.290053606033325\n",
      "2.289888858795166\n",
      "2.289724588394165\n",
      "2.2895610332489014\n",
      "2.289398193359375\n",
      "2.2892355918884277\n",
      "2.289073944091797\n",
      "2.288912534713745\n",
      "step 480: train loss 2.2889, val loss 2.3166\n",
      "2.2887520790100098\n",
      "2.2885921001434326\n",
      "2.2884328365325928\n",
      "2.288274049758911\n",
      "2.2881157398223877\n",
      "2.2879581451416016\n",
      "2.2878010272979736\n",
      "2.287644624710083\n",
      "2.2874886989593506\n",
      "2.2873332500457764\n",
      "step 490: train loss 2.2873, val loss 2.3151\n",
      "2.2871785163879395\n",
      "2.2870242595672607\n",
      "2.2868707180023193\n",
      "2.2867178916931152\n",
      "2.2865653038024902\n",
      "2.2864134311676025\n",
      "2.286261796951294\n",
      "2.2861108779907227\n",
      "2.2859609127044678\n",
      "2.285810947418213\n",
      "step 500: train loss 2.2858, val loss 2.3137\n",
      "2.2856616973876953\n",
      "2.285512685775757\n",
      "2.285364866256714\n",
      "2.285217046737671\n",
      "2.2850699424743652\n",
      "2.2849233150482178\n",
      "2.2847771644592285\n",
      "2.2846317291259766\n",
      "2.2844865322113037\n",
      "2.284342050552368\n",
      "step 510: train loss 2.2843, val loss 2.3124\n",
      "2.284198045730591\n",
      "2.2840545177459717\n",
      "2.28391170501709\n",
      "2.283769130706787\n",
      "2.2836267948150635\n",
      "2.2834854125976562\n",
      "2.283344268798828\n",
      "2.2832038402557373\n",
      "2.2830638885498047\n",
      "2.2829244136810303\n",
      "step 520: train loss 2.2829, val loss 2.3111\n",
      "2.282785177230835\n",
      "2.282646656036377\n",
      "2.282508611679077\n",
      "2.2823708057403564\n",
      "2.282233476638794\n",
      "2.2820968627929688\n",
      "2.2819604873657227\n",
      "2.281824827194214\n",
      "2.2816896438598633\n",
      "2.281554698944092\n",
      "step 530: train loss 2.2816, val loss 2.3099\n",
      "2.2814204692840576\n",
      "2.2812862396240234\n",
      "2.2811529636383057\n",
      "2.281020164489746\n",
      "2.2808876037597656\n",
      "2.280755043029785\n",
      "2.280623435974121\n",
      "2.2804923057556152\n",
      "2.2803616523742676\n",
      "2.280231237411499\n",
      "step 540: train loss 2.2802, val loss 2.3087\n",
      "2.2801010608673096\n",
      "2.2799715995788574\n",
      "2.2798426151275635\n",
      "2.2797138690948486\n",
      "2.279585361480713\n",
      "2.2794580459594727\n",
      "2.2793307304382324\n",
      "2.2792036533355713\n",
      "2.2790772914886475\n",
      "2.2789509296417236\n",
      "step 550: train loss 2.2790, val loss 2.3075\n",
      "2.278825283050537\n",
      "2.278700113296509\n",
      "2.2785754203796387\n",
      "2.2784507274627686\n",
      "2.2783267498016357\n",
      "2.278202772140503\n",
      "2.2780797481536865\n",
      "2.277956962585449\n",
      "2.277834415435791\n",
      "2.277712345123291\n",
      "step 560: train loss 2.2777, val loss 2.3064\n",
      "2.27759051322937\n",
      "2.2774691581726074\n",
      "2.277348518371582\n",
      "2.2772278785705566\n",
      "2.2771077156066895\n",
      "2.2769882678985596\n",
      "2.2768685817718506\n",
      "2.276749610900879\n",
      "2.2766308784484863\n",
      "2.27651309967041\n",
      "step 570: train loss 2.2765, val loss 2.3053\n",
      "2.276395082473755\n",
      "2.2762773036956787\n",
      "2.27616024017334\n",
      "2.2760441303253174\n",
      "2.2759275436401367\n",
      "2.2758114337921143\n",
      "2.275695562362671\n",
      "2.2755801677703857\n",
      "2.275465488433838\n",
      "2.275351047515869\n",
      "step 580: train loss 2.2754, val loss 2.3042\n",
      "2.2752370834350586\n",
      "2.275123119354248\n",
      "2.2750096321105957\n",
      "2.2748966217041016\n",
      "2.2747838497161865\n",
      "2.2746713161468506\n",
      "2.2745590209960938\n",
      "2.274447202682495\n",
      "2.2743358612060547\n",
      "2.2742249965667725\n",
      "step 590: train loss 2.2742, val loss 2.3032\n",
      "2.2741141319274902\n",
      "2.274003744125366\n",
      "2.2738935947418213\n",
      "2.2737841606140137\n",
      "2.273674726486206\n",
      "2.2735657691955566\n",
      "2.2734570503234863\n",
      "2.273348569869995\n",
      "2.273240566253662\n",
      "2.273132801055908\n",
      "step 600: train loss 2.2731, val loss 2.3022\n",
      "2.2730252742767334\n",
      "2.272918462753296\n",
      "2.2728116512298584\n",
      "2.272705316543579\n",
      "2.272599220275879\n",
      "2.272493362426758\n",
      "2.272387981414795\n",
      "2.272282600402832\n",
      "2.2721779346466064\n",
      "2.27207350730896\n",
      "step 610: train loss 2.2721, val loss 2.3013\n",
      "2.2719690799713135\n",
      "2.271865129470825\n",
      "2.271761655807495\n",
      "2.271658182144165\n",
      "2.271555185317993\n",
      "2.2714526653289795\n",
      "2.271350383758545\n",
      "2.2712483406066895\n",
      "2.271146297454834\n",
      "2.271044969558716\n",
      "step 620: train loss 2.2710, val loss 2.3004\n",
      "2.2709436416625977\n",
      "2.270843267440796\n",
      "2.270742177963257\n",
      "2.270641803741455\n",
      "2.2705419063568115\n",
      "2.270442247390747\n",
      "2.270343065261841\n",
      "2.2702438831329346\n",
      "2.2701449394226074\n",
      "2.2700464725494385\n",
      "step 630: train loss 2.2700, val loss 2.2995\n",
      "2.2699480056762695\n",
      "2.269850015640259\n",
      "2.2697525024414062\n",
      "2.2696549892425537\n",
      "2.2695579528808594\n",
      "2.269460916519165\n",
      "2.269364356994629\n",
      "2.269268274307251\n",
      "2.269171953201294\n",
      "2.269076347351074\n",
      "step 640: train loss 2.2691, val loss 2.2986\n",
      "2.2689807415008545\n",
      "2.268885374069214\n",
      "2.2687907218933105\n",
      "2.268695831298828\n",
      "2.268601417541504\n",
      "2.2685070037841797\n",
      "2.268413543701172\n",
      "2.268319845199585\n",
      "2.268226385116577\n",
      "2.2681331634521484\n",
      "step 650: train loss 2.2681, val loss 2.2978\n",
      "2.268040895462036\n",
      "2.2679481506347656\n",
      "2.267855644226074\n",
      "2.26776385307312\n",
      "2.267671585083008\n",
      "2.267580032348633\n",
      "2.267488956451416\n",
      "2.267397880554199\n",
      "2.2673072814941406\n",
      "2.267216682434082\n",
      "step 660: train loss 2.2672, val loss 2.2970\n",
      "2.2671263217926025\n",
      "2.267036199569702\n",
      "2.266946315765381\n",
      "2.266857147216797\n",
      "2.266767740249634\n",
      "2.266678810119629\n",
      "2.266589879989624\n",
      "2.2665011882781982\n",
      "2.2664129734039307\n",
      "2.266324758529663\n",
      "step 670: train loss 2.2663, val loss 2.2962\n",
      "2.2662370204925537\n",
      "2.2661492824554443\n",
      "2.266062021255493\n",
      "2.265974998474121\n",
      "2.265887975692749\n",
      "2.265801429748535\n",
      "2.2657148838043213\n",
      "2.2656288146972656\n",
      "2.26554274559021\n",
      "2.2654571533203125\n",
      "step 680: train loss 2.2655, val loss 2.2954\n",
      "2.265371799468994\n",
      "2.2652862071990967\n",
      "2.2652013301849365\n",
      "2.2651166915893555\n",
      "2.2650320529937744\n",
      "2.2649476528167725\n",
      "2.2648637294769287\n",
      "2.264779806137085\n",
      "2.2646961212158203\n",
      "2.264612913131714\n",
      "step 690: train loss 2.2646, val loss 2.2947\n",
      "2.264529228210449\n",
      "2.264446258544922\n",
      "2.2643635272979736\n",
      "2.2642810344696045\n",
      "2.2641987800598145\n",
      "2.2641165256500244\n",
      "2.2640345096588135\n",
      "2.2639529705047607\n",
      "2.263871192932129\n",
      "2.2637901306152344\n",
      "step 700: train loss 2.2638, val loss 2.2939\n",
      "2.2637088298797607\n",
      "2.2636282444000244\n",
      "2.263547420501709\n",
      "2.263467311859131\n",
      "2.2633867263793945\n",
      "2.2633068561553955\n",
      "2.2632269859313965\n",
      "2.2631475925445557\n",
      "2.263068199157715\n",
      "2.262989044189453\n",
      "step 710: train loss 2.2630, val loss 2.2932\n",
      "2.2629103660583496\n",
      "2.262830972671509\n",
      "2.2627527713775635\n",
      "2.26267409324646\n",
      "2.262596368789673\n",
      "2.2625181674957275\n",
      "2.2624402046203613\n",
      "2.2623627185821533\n",
      "2.2622852325439453\n",
      "2.2622082233428955\n",
      "step 720: train loss 2.2622, val loss 2.2925\n",
      "2.262131452560425\n",
      "2.262054681777954\n",
      "2.2619779109954834\n",
      "2.261901617050171\n",
      "2.2618255615234375\n",
      "2.261749505996704\n",
      "2.26167368888855\n",
      "2.2615978717803955\n",
      "2.2615225315093994\n",
      "2.2614474296569824\n",
      "step 730: train loss 2.2614, val loss 2.2919\n",
      "2.2613723278045654\n",
      "2.2612974643707275\n",
      "2.2612226009368896\n",
      "2.26114821434021\n",
      "2.2610743045806885\n",
      "2.2609996795654297\n",
      "2.260925769805908\n",
      "2.260852098464966\n",
      "2.2607786655426025\n",
      "2.2607052326202393\n",
      "step 740: train loss 2.2607, val loss 2.2912\n",
      "2.260632276535034\n",
      "2.26055908203125\n",
      "2.260486364364624\n",
      "2.260413646697998\n",
      "2.260341167449951\n",
      "2.2602689266204834\n",
      "2.2601969242095947\n",
      "2.260124683380127\n",
      "2.2600529193878174\n",
      "2.259981393814087\n",
      "step 750: train loss 2.2600, val loss 2.2906\n",
      "2.2599101066589355\n",
      "2.259838819503784\n",
      "2.259767770767212\n",
      "2.2596969604492188\n",
      "2.259626626968384\n",
      "2.2595555782318115\n",
      "2.2594854831695557\n",
      "2.2594151496887207\n",
      "2.259345054626465\n",
      "2.259275436401367\n",
      "step 760: train loss 2.2593, val loss 2.2900\n",
      "2.2592060565948486\n",
      "2.259136199951172\n",
      "2.2590668201446533\n",
      "2.258997678756714\n",
      "2.2589285373687744\n",
      "2.2588601112365723\n",
      "2.258791208267212\n",
      "2.2587227821350098\n",
      "2.2586543560028076\n",
      "2.2585861682891846\n",
      "step 770: train loss 2.2586, val loss 2.2894\n",
      "2.2585179805755615\n",
      "2.2584502696990967\n",
      "2.258382797241211\n",
      "2.258314847946167\n",
      "2.2582478523254395\n",
      "2.2581803798675537\n",
      "2.258113384246826\n",
      "2.258046865463257\n",
      "2.2579798698425293\n",
      "2.25791335105896\n",
      "step 780: train loss 2.2579, val loss 2.2888\n",
      "2.2578468322753906\n",
      "2.2577803134918213\n",
      "2.2577145099639893\n",
      "2.2576487064361572\n",
      "2.257582902908325\n",
      "2.257517099380493\n",
      "2.2574515342712402\n",
      "2.2573866844177246\n",
      "2.2573211193084717\n",
      "2.257256507873535\n",
      "step 790: train loss 2.2573, val loss 2.2882\n",
      "2.2571916580200195\n",
      "2.257126808166504\n",
      "2.2570624351501465\n",
      "2.25699782371521\n",
      "2.2569336891174316\n",
      "2.256869316101074\n",
      "2.256805658340454\n",
      "2.256741762161255\n",
      "2.2566781044006348\n",
      "Reg strength: 0.000, Train loss: 2.2567, Val loss: 2.2882\n",
      "3.748319149017334\n",
      "step 0: train loss 3.7483, val loss 3.7543\n",
      "3.6344263553619385\n",
      "3.550145387649536\n",
      "3.484727382659912\n",
      "3.429079294204712\n",
      "3.3790476322174072\n",
      "3.3334155082702637\n",
      "3.2915987968444824\n",
      "3.253203868865967\n",
      "3.2179064750671387\n",
      "3.185405731201172\n",
      "step 10: train loss 3.1854, val loss 3.1927\n",
      "3.155413866043091\n",
      "3.1276533603668213\n",
      "3.1018667221069336\n",
      "3.0778205394744873\n",
      "3.0553109645843506\n",
      "3.034165382385254\n",
      "3.014239549636841\n",
      "2.9954140186309814\n",
      "2.9775891304016113\n",
      "2.960679054260254\n",
      "step 20: train loss 2.9607, val loss 2.9687\n",
      "2.944612503051758\n",
      "2.929326057434082\n",
      "2.9147629737854004\n",
      "2.9008727073669434\n",
      "2.8876099586486816\n",
      "2.8749325275421143\n",
      "2.862802743911743\n",
      "2.851184844970703\n",
      "2.8400468826293945\n",
      "2.8293585777282715\n",
      "step 30: train loss 2.8294, val loss 2.8381\n",
      "2.819092273712158\n",
      "2.8092219829559326\n",
      "2.7997238636016846\n",
      "2.7905750274658203\n",
      "2.781755208969116\n",
      "2.7732458114624023\n",
      "2.7650275230407715\n",
      "2.757084846496582\n",
      "2.7494020462036133\n",
      "2.741964340209961\n",
      "step 40: train loss 2.7420, val loss 2.7521\n",
      "2.7347593307495117\n",
      "2.7277746200561523\n",
      "2.7209982872009277\n",
      "2.7144196033477783\n",
      "2.708029270172119\n",
      "2.7018179893493652\n",
      "2.6957767009735107\n",
      "2.6898982524871826\n",
      "2.6841742992401123\n",
      "2.678598642349243\n",
      "step 50: train loss 2.6786, val loss 2.6902\n",
      "2.6731646060943604\n",
      "2.6678664684295654\n",
      "2.6626980304718018\n",
      "2.65765380859375\n",
      "2.6527297496795654\n",
      "2.6479198932647705\n",
      "2.643221139907837\n",
      "2.6386282444000244\n",
      "2.6341378688812256\n",
      "2.6297459602355957\n",
      "step 60: train loss 2.6297, val loss 2.6427\n",
      "2.6254496574401855\n",
      "2.6212451457977295\n",
      "2.61712908744812\n",
      "2.6130993366241455\n",
      "2.609152317047119\n",
      "2.605285882949829\n",
      "2.601496934890747\n",
      "2.597783327102661\n",
      "2.5941429138183594\n",
      "2.5905730724334717\n",
      "step 70: train loss 2.5906, val loss 2.6046\n",
      "2.5870726108551025\n",
      "2.5836384296417236\n",
      "2.5802688598632812\n",
      "2.576962471008301\n",
      "2.5737175941467285\n",
      "2.5705316066741943\n",
      "2.567403554916382\n",
      "2.5643320083618164\n",
      "2.5613152980804443\n",
      "2.5583510398864746\n",
      "step 80: train loss 2.5584, val loss 2.5733\n",
      "2.5554392337799072\n",
      "2.5525779724121094\n",
      "2.5497658252716064\n",
      "2.547001361846924\n",
      "2.544283390045166\n",
      "2.541611433029175\n",
      "2.5389838218688965\n",
      "2.5363991260528564\n",
      "2.5338566303253174\n",
      "2.5313549041748047\n",
      "step 90: train loss 2.5314, val loss 2.5471\n",
      "2.52889347076416\n",
      "2.526470899581909\n",
      "2.5240867137908936\n",
      "2.5217397212982178\n",
      "2.5194287300109863\n",
      "2.517153263092041\n",
      "2.5149123668670654\n",
      "2.5127053260803223\n",
      "2.510530948638916\n",
      "2.5083887577056885\n",
      "step 100: train loss 2.5084, val loss 2.5249\n",
      "2.5062780380249023\n",
      "2.5041980743408203\n",
      "2.502147912979126\n",
      "2.5001273155212402\n",
      "2.4981348514556885\n",
      "2.49617075920105\n",
      "2.4942336082458496\n",
      "2.492323637008667\n",
      "2.4904396533966064\n",
      "2.4885809421539307\n",
      "step 110: train loss 2.4886, val loss 2.5057\n",
      "2.4867477416992188\n",
      "2.484938621520996\n",
      "2.483153820037842\n",
      "2.4813923835754395\n",
      "2.4796535968780518\n",
      "2.477937698364258\n",
      "2.476243019104004\n",
      "2.4745705127716064\n",
      "2.472919225692749\n",
      "2.4712882041931152\n",
      "step 120: train loss 2.4713, val loss 2.4891\n",
      "2.469677448272705\n",
      "2.4680871963500977\n",
      "2.46651554107666\n",
      "2.464963674545288\n",
      "2.4634299278259277\n",
      "2.4619152545928955\n",
      "2.460418462753296\n",
      "2.4589388370513916\n",
      "2.45747709274292\n",
      "2.4560320377349854\n",
      "step 130: train loss 2.4560, val loss 2.4744\n",
      "2.454603910446167\n",
      "2.4531919956207275\n",
      "2.451796531677246\n",
      "2.4504165649414062\n",
      "2.449052572250366\n",
      "2.4477038383483887\n",
      "2.4463696479797363\n",
      "2.4450504779815674\n",
      "2.4437460899353027\n",
      "2.442455768585205\n",
      "step 140: train loss 2.4425, val loss 2.4613\n",
      "2.441179037094116\n",
      "2.4399168491363525\n",
      "2.4386675357818604\n",
      "2.437432050704956\n",
      "2.4362096786499023\n",
      "2.434999942779541\n",
      "2.433803081512451\n",
      "2.4326186180114746\n",
      "2.4314465522766113\n",
      "2.430286407470703\n",
      "step 150: train loss 2.4303, val loss 2.4497\n",
      "2.429138660430908\n",
      "2.42800235748291\n",
      "2.42687726020813\n",
      "2.4257640838623047\n",
      "2.424661636352539\n",
      "2.4235706329345703\n",
      "2.422490119934082\n",
      "2.4214208126068115\n",
      "2.4203615188598633\n",
      "2.4193129539489746\n",
      "step 160: train loss 2.4193, val loss 2.4392\n",
      "2.4182746410369873\n",
      "2.417246103286743\n",
      "2.4162278175354004\n",
      "2.415219306945801\n",
      "2.4142203330993652\n",
      "2.413231134414673\n",
      "2.4122509956359863\n",
      "2.411280393600464\n",
      "2.4103188514709473\n",
      "2.4093658924102783\n",
      "step 170: train loss 2.4094, val loss 2.4297\n",
      "2.4084219932556152\n",
      "2.407486915588379\n",
      "2.4065608978271484\n",
      "2.4056429862976074\n",
      "2.404733180999756\n",
      "2.403832197189331\n",
      "2.4029390811920166\n",
      "2.4020540714263916\n",
      "2.401176929473877\n",
      "2.4003076553344727\n",
      "step 180: train loss 2.4003, val loss 2.4211\n",
      "2.3994462490081787\n",
      "2.398592233657837\n",
      "2.3977463245391846\n",
      "2.396907329559326\n",
      "2.39607572555542\n",
      "2.395251750946045\n",
      "2.394434690475464\n",
      "2.3936245441436768\n",
      "2.3928213119506836\n",
      "2.3920249938964844\n",
      "step 190: train loss 2.3920, val loss 2.4132\n",
      "2.391235589981079\n",
      "2.3904528617858887\n",
      "2.389677047729492\n",
      "2.3889071941375732\n",
      "2.388144016265869\n",
      "2.38738751411438\n",
      "2.3866372108459473\n",
      "2.385892629623413\n",
      "2.3851547241210938\n",
      "2.384422540664673\n",
      "step 200: train loss 2.3844, val loss 2.4060\n",
      "2.3836965560913086\n",
      "2.382976531982422\n",
      "2.3822622299194336\n",
      "2.3815536499023438\n",
      "2.3808507919311523\n",
      "2.3801538944244385\n",
      "2.379462242126465\n",
      "2.3787760734558105\n",
      "2.3780951499938965\n",
      "2.37742018699646\n",
      "step 210: train loss 2.3774, val loss 2.3993\n",
      "2.3767499923706055\n",
      "2.3760855197906494\n",
      "2.3754255771636963\n",
      "2.3747713565826416\n",
      "2.374121904373169\n",
      "2.3734774589538574\n",
      "2.372838020324707\n",
      "2.3722035884857178\n",
      "2.3715741634368896\n",
      "2.3709487915039062\n",
      "step 220: train loss 2.3709, val loss 2.3932\n",
      "2.370328903198242\n",
      "2.369713306427002\n",
      "2.3691024780273438\n",
      "2.3684959411621094\n",
      "2.367893934249878\n",
      "2.3672964572906494\n",
      "2.366703748703003\n",
      "2.366114616394043\n",
      "2.365530490875244\n",
      "2.364950180053711\n",
      "step 230: train loss 2.3650, val loss 2.3875\n",
      "2.3643743991851807\n",
      "2.363802671432495\n",
      "2.3632349967956543\n",
      "2.3626718521118164\n",
      "2.362112045288086\n",
      "2.3615562915802\n",
      "2.3610050678253174\n",
      "2.360457181930542\n",
      "2.3599138259887695\n",
      "2.3593735694885254\n",
      "step 240: train loss 2.3594, val loss 2.3823\n",
      "2.358837604522705\n",
      "2.358304977416992\n",
      "2.357776403427124\n",
      "2.3572511672973633\n",
      "2.356729507446289\n",
      "2.3562116622924805\n",
      "2.3556973934173584\n",
      "2.3551864624023438\n",
      "2.3546793460845947\n",
      "2.354175090789795\n",
      "step 250: train loss 2.3542, val loss 2.3774\n",
      "2.3536744117736816\n",
      "2.353177547454834\n",
      "2.3526835441589355\n",
      "2.3521928787231445\n",
      "2.351705551147461\n",
      "2.3512213230133057\n",
      "2.350740671157837\n",
      "2.3502628803253174\n",
      "2.349788188934326\n",
      "2.3493165969848633\n",
      "step 260: train loss 2.3493, val loss 2.3728\n",
      "2.348848342895508\n",
      "2.3483827114105225\n",
      "2.3479204177856445\n",
      "2.347460985183716\n",
      "2.3470048904418945\n",
      "2.346550703048706\n",
      "2.3461005687713623\n",
      "2.3456525802612305\n",
      "2.345207452774048\n",
      "2.3447651863098145\n",
      "step 270: train loss 2.3448, val loss 2.3685\n",
      "2.3443257808685303\n",
      "2.3438894748687744\n",
      "2.3434555530548096\n",
      "2.3430237770080566\n",
      "2.3425955772399902\n",
      "2.3421695232391357\n",
      "2.3417465686798096\n",
      "2.3413257598876953\n",
      "2.340907573699951\n",
      "2.3404922485351562\n",
      "step 280: train loss 2.3405, val loss 2.3645\n",
      "2.3400790691375732\n",
      "2.3396685123443604\n",
      "2.3392605781555176\n",
      "2.338855028152466\n",
      "2.338451862335205\n",
      "2.3380510807037354\n",
      "2.3376526832580566\n",
      "2.337257146835327\n",
      "2.3368632793426514\n",
      "2.3364717960357666\n",
      "step 290: train loss 2.3365, val loss 2.3607\n",
      "2.336082935333252\n",
      "2.33569598197937\n",
      "2.3353116512298584\n",
      "2.3349294662475586\n",
      "2.334549903869629\n",
      "2.334171772003174\n",
      "2.3337960243225098\n",
      "2.3334226608276367\n",
      "2.3330512046813965\n",
      "2.3326823711395264\n",
      "step 300: train loss 2.3327, val loss 2.3572\n",
      "2.332314968109131\n",
      "2.3319504261016846\n",
      "2.331587314605713\n",
      "2.3312265872955322\n",
      "2.3308680057525635\n",
      "2.3305110931396484\n",
      "2.3301565647125244\n",
      "2.329803466796875\n",
      "2.3294527530670166\n",
      "2.329103946685791\n",
      "step 310: train loss 2.3291, val loss 2.3538\n",
      "2.3287570476531982\n",
      "2.3284120559692383\n",
      "2.328068733215332\n",
      "2.327727794647217\n",
      "2.3273885250091553\n",
      "2.3270506858825684\n",
      "2.3267149925231934\n",
      "2.3263814449310303\n",
      "2.326049327850342\n",
      "2.325718879699707\n",
      "step 320: train loss 2.3257, val loss 2.3507\n",
      "2.3253908157348633\n",
      "2.325063943862915\n",
      "2.3247389793395996\n",
      "2.324415922164917\n",
      "2.324094295501709\n",
      "2.323774814605713\n",
      "2.3234565258026123\n",
      "2.3231403827667236\n",
      "2.3228254318237305\n",
      "2.322512626647949\n",
      "step 330: train loss 2.3225, val loss 2.3477\n",
      "2.3222012519836426\n",
      "2.3218917846679688\n",
      "2.3215832710266113\n",
      "2.321276903152466\n",
      "2.320971727371216\n",
      "2.3206684589385986\n",
      "2.320366859436035\n",
      "2.320066452026367\n",
      "2.319767475128174\n",
      "2.3194706439971924\n",
      "step 340: train loss 2.3195, val loss 2.3449\n",
      "2.3191750049591064\n",
      "2.318881034851074\n",
      "2.3185882568359375\n",
      "2.3182971477508545\n",
      "2.318007230758667\n",
      "2.317718744277954\n",
      "2.317432403564453\n",
      "2.3171470165252686\n",
      "2.3168630599975586\n",
      "2.3165805339813232\n",
      "step 350: train loss 2.3166, val loss 2.3422\n",
      "2.3162994384765625\n",
      "2.3160197734832764\n",
      "2.3157413005828857\n",
      "2.315464496612549\n",
      "2.3151888847351074\n",
      "2.3149147033691406\n",
      "2.3146421909332275\n",
      "2.3143701553344727\n",
      "2.314100503921509\n",
      "2.313831090927124\n",
      "step 360: train loss 2.3138, val loss 2.3396\n",
      "2.313563823699951\n",
      "2.3132975101470947\n",
      "2.313032627105713\n",
      "2.3127686977386475\n",
      "2.3125061988830566\n",
      "2.3122448921203613\n",
      "2.311985492706299\n",
      "2.3117268085479736\n",
      "2.311469078063965\n",
      "2.3112130165100098\n",
      "step 370: train loss 2.3112, val loss 2.3372\n",
      "2.310957670211792\n",
      "2.310703992843628\n",
      "2.3104512691497803\n",
      "2.3101999759674072\n",
      "2.3099498748779297\n",
      "2.3097007274627686\n",
      "2.309452533721924\n",
      "2.309206008911133\n",
      "2.308960199356079\n",
      "2.3087158203125\n",
      "step 380: train loss 2.3087, val loss 2.3348\n",
      "2.3084723949432373\n",
      "2.308230400085449\n",
      "2.3079893589019775\n",
      "2.3077495098114014\n",
      "2.3075106143951416\n",
      "2.3072726726531982\n",
      "2.3070361614227295\n",
      "2.306800603866577\n",
      "2.306565761566162\n",
      "2.3063321113586426\n",
      "step 390: train loss 2.3063, val loss 2.3326\n",
      "2.3060998916625977\n",
      "2.30586838722229\n",
      "2.305638074874878\n",
      "2.3054089546203613\n",
      "2.305180549621582\n",
      "2.3049533367156982\n",
      "2.304727077484131\n",
      "2.30450177192688\n",
      "2.3042776584625244\n",
      "2.3040544986724854\n",
      "step 400: train loss 2.3041, val loss 2.3305\n",
      "2.3038320541381836\n",
      "2.3036108016967773\n",
      "2.3033907413482666\n",
      "2.303171396255493\n",
      "2.302952527999878\n",
      "2.3027353286743164\n",
      "2.3025190830230713\n",
      "2.3023033142089844\n",
      "2.302088975906372\n",
      "2.301875114440918\n",
      "step 410: train loss 2.3019, val loss 2.3285\n",
      "2.3016622066497803\n",
      "2.301450490951538\n",
      "2.3012397289276123\n",
      "2.301029682159424\n",
      "2.300820827484131\n",
      "2.300612688064575\n",
      "2.300405263900757\n",
      "2.300198793411255\n",
      "2.2999930381774902\n",
      "2.299788475036621\n",
      "step 420: train loss 2.2998, val loss 2.3266\n",
      "2.2995846271514893\n",
      "2.2993812561035156\n",
      "2.2991795539855957\n",
      "2.298978328704834\n",
      "2.2987778186798096\n",
      "2.2985782623291016\n",
      "2.298379421234131\n",
      "2.2981815338134766\n",
      "2.2979843616485596\n",
      "2.297788143157959\n",
      "step 430: train loss 2.2978, val loss 2.3247\n",
      "2.2975926399230957\n",
      "2.2973978519439697\n",
      "2.2972044944763184\n",
      "2.297011137008667\n",
      "2.296818733215332\n",
      "2.2966272830963135\n",
      "2.2964365482330322\n",
      "2.2962465286254883\n",
      "2.2960574626922607\n",
      "2.2958691120147705\n",
      "step 440: train loss 2.2959, val loss 2.3230\n",
      "2.2956814765930176\n",
      "2.295494556427002\n",
      "2.2953083515167236\n",
      "2.2951231002807617\n",
      "2.294938325881958\n",
      "2.29475474357605\n",
      "2.2945716381073\n",
      "2.294389009475708\n",
      "2.2942073345184326\n",
      "2.2940263748168945\n",
      "step 450: train loss 2.2940, val loss 2.3213\n",
      "2.293846368789673\n",
      "2.2936668395996094\n",
      "2.293487787246704\n",
      "2.2933096885681152\n",
      "2.2931320667266846\n",
      "2.2929553985595703\n",
      "2.2927796840667725\n",
      "2.292604446411133\n",
      "2.2924294471740723\n",
      "2.2922556400299072\n",
      "step 460: train loss 2.2923, val loss 2.3196\n",
      "2.2920823097229004\n",
      "2.2919094562530518\n",
      "2.2917380332946777\n",
      "2.2915663719177246\n",
      "2.291395664215088\n",
      "2.2912256717681885\n",
      "2.2910563945770264\n",
      "2.2908878326416016\n",
      "2.290719747543335\n",
      "2.2905526161193848\n",
      "step 470: train loss 2.2906, val loss 2.3181\n",
      "2.2903854846954346\n",
      "2.29021954536438\n",
      "2.290053606033325\n",
      "2.289888858795166\n",
      "2.289724588394165\n",
      "2.2895610332489014\n",
      "2.289398193359375\n",
      "2.2892355918884277\n",
      "2.289073944091797\n",
      "2.288912534713745\n",
      "step 480: train loss 2.2889, val loss 2.3166\n",
      "2.2887520790100098\n",
      "2.2885921001434326\n",
      "2.2884328365325928\n",
      "2.288274049758911\n",
      "2.2881157398223877\n",
      "2.2879581451416016\n",
      "2.2878010272979736\n",
      "2.287644624710083\n",
      "2.2874886989593506\n",
      "2.2873332500457764\n",
      "step 490: train loss 2.2873, val loss 2.3151\n",
      "2.2871785163879395\n",
      "2.2870242595672607\n",
      "2.2868707180023193\n",
      "2.2867178916931152\n",
      "2.2865653038024902\n",
      "2.2864134311676025\n",
      "2.286261796951294\n",
      "2.2861108779907227\n",
      "2.2859609127044678\n",
      "2.285810947418213\n",
      "step 500: train loss 2.2858, val loss 2.3137\n",
      "2.2856616973876953\n",
      "2.285512685775757\n",
      "2.285364866256714\n",
      "2.285217046737671\n",
      "2.2850699424743652\n",
      "2.2849233150482178\n",
      "2.2847771644592285\n",
      "2.2846317291259766\n",
      "2.2844865322113037\n",
      "2.284342050552368\n",
      "step 510: train loss 2.2843, val loss 2.3124\n",
      "2.284198045730591\n",
      "2.2840545177459717\n",
      "2.28391170501709\n",
      "2.283769130706787\n",
      "2.2836267948150635\n",
      "2.2834854125976562\n",
      "2.283344268798828\n",
      "2.2832038402557373\n",
      "2.2830638885498047\n",
      "2.2829244136810303\n",
      "step 520: train loss 2.2829, val loss 2.3111\n",
      "2.282785177230835\n",
      "2.282646656036377\n",
      "2.282508611679077\n",
      "2.2823708057403564\n",
      "2.282233476638794\n",
      "2.2820968627929688\n",
      "2.2819604873657227\n",
      "2.281824827194214\n",
      "2.2816896438598633\n",
      "2.281554698944092\n",
      "step 530: train loss 2.2816, val loss 2.3099\n",
      "2.2814204692840576\n",
      "2.2812862396240234\n",
      "2.2811529636383057\n",
      "2.281020164489746\n",
      "2.2808876037597656\n",
      "2.280755043029785\n",
      "2.280623435974121\n",
      "2.2804923057556152\n",
      "2.2803616523742676\n",
      "2.280231237411499\n",
      "step 540: train loss 2.2802, val loss 2.3087\n",
      "2.2801010608673096\n",
      "2.2799715995788574\n",
      "2.2798426151275635\n",
      "2.2797138690948486\n",
      "2.279585361480713\n",
      "2.2794580459594727\n",
      "2.2793307304382324\n",
      "2.2792036533355713\n",
      "2.2790772914886475\n",
      "2.2789509296417236\n",
      "step 550: train loss 2.2790, val loss 2.3075\n",
      "2.278825283050537\n",
      "2.278700113296509\n",
      "2.2785754203796387\n",
      "2.2784507274627686\n",
      "2.2783267498016357\n",
      "2.278202772140503\n",
      "2.2780797481536865\n",
      "2.277956962585449\n",
      "2.277834415435791\n",
      "2.277712345123291\n",
      "step 560: train loss 2.2777, val loss 2.3064\n",
      "2.27759051322937\n",
      "2.2774691581726074\n",
      "2.277348518371582\n",
      "2.2772278785705566\n",
      "2.2771077156066895\n",
      "2.2769882678985596\n",
      "2.2768685817718506\n",
      "2.276749610900879\n",
      "2.2766308784484863\n",
      "2.27651309967041\n",
      "step 570: train loss 2.2765, val loss 2.3053\n",
      "2.276395082473755\n",
      "2.2762773036956787\n",
      "2.27616024017334\n",
      "2.2760441303253174\n",
      "2.2759275436401367\n",
      "2.2758114337921143\n",
      "2.275695562362671\n",
      "2.2755801677703857\n",
      "2.275465488433838\n",
      "2.275351047515869\n",
      "step 580: train loss 2.2754, val loss 2.3042\n",
      "2.2752370834350586\n",
      "2.275123119354248\n",
      "2.2750096321105957\n",
      "2.2748966217041016\n",
      "2.2747838497161865\n",
      "2.2746713161468506\n",
      "2.2745590209960938\n",
      "2.274447202682495\n",
      "2.2743358612060547\n",
      "2.2742249965667725\n",
      "step 590: train loss 2.2742, val loss 2.3032\n",
      "2.2741141319274902\n",
      "2.274003744125366\n",
      "2.2738935947418213\n",
      "2.2737841606140137\n",
      "2.273674726486206\n",
      "2.2735657691955566\n",
      "2.2734570503234863\n",
      "2.273348569869995\n",
      "2.273240566253662\n",
      "2.273132801055908\n",
      "step 600: train loss 2.2731, val loss 2.3022\n",
      "2.2730252742767334\n",
      "2.272918462753296\n",
      "2.2728116512298584\n",
      "2.272705316543579\n",
      "2.272599220275879\n",
      "2.272493362426758\n",
      "2.272387981414795\n",
      "2.272282600402832\n",
      "2.2721779346466064\n",
      "2.27207350730896\n",
      "step 610: train loss 2.2721, val loss 2.3013\n",
      "2.2719690799713135\n",
      "2.271865129470825\n",
      "2.271761655807495\n",
      "2.271658182144165\n",
      "2.271555185317993\n",
      "2.2714526653289795\n",
      "2.271350383758545\n",
      "2.2712483406066895\n",
      "2.271146297454834\n",
      "2.271044969558716\n",
      "step 620: train loss 2.2710, val loss 2.3004\n",
      "2.2709436416625977\n",
      "2.270843267440796\n",
      "2.270742177963257\n",
      "2.270641803741455\n",
      "2.2705419063568115\n",
      "2.270442247390747\n",
      "2.270343065261841\n",
      "2.2702438831329346\n",
      "2.2701449394226074\n",
      "2.2700464725494385\n",
      "step 630: train loss 2.2700, val loss 2.2995\n",
      "2.2699480056762695\n",
      "2.269850015640259\n",
      "2.2697525024414062\n",
      "2.2696549892425537\n",
      "2.2695579528808594\n",
      "2.269460916519165\n",
      "2.269364356994629\n",
      "2.269268274307251\n",
      "2.269171953201294\n",
      "2.269076347351074\n",
      "step 640: train loss 2.2691, val loss 2.2986\n",
      "2.2689807415008545\n",
      "2.268885374069214\n",
      "2.2687907218933105\n",
      "2.268695831298828\n",
      "2.268601417541504\n",
      "2.2685070037841797\n",
      "2.268413543701172\n",
      "2.268319845199585\n",
      "2.268226385116577\n",
      "2.2681331634521484\n",
      "step 650: train loss 2.2681, val loss 2.2978\n",
      "2.268040895462036\n",
      "2.2679481506347656\n",
      "2.267855644226074\n",
      "2.26776385307312\n",
      "2.267671585083008\n",
      "2.267580032348633\n",
      "2.267488956451416\n",
      "2.267397880554199\n",
      "2.2673072814941406\n",
      "2.267216682434082\n",
      "step 660: train loss 2.2672, val loss 2.2970\n",
      "2.2671263217926025\n",
      "2.267036199569702\n",
      "2.266946315765381\n",
      "2.266857147216797\n",
      "2.266767740249634\n",
      "2.266678810119629\n",
      "2.266589879989624\n",
      "2.2665011882781982\n",
      "2.2664129734039307\n",
      "2.266324758529663\n",
      "step 670: train loss 2.2663, val loss 2.2962\n",
      "2.2662370204925537\n",
      "2.2661492824554443\n",
      "2.266062021255493\n",
      "2.265974998474121\n",
      "2.265887975692749\n",
      "2.265801429748535\n",
      "2.2657148838043213\n",
      "2.2656288146972656\n",
      "2.26554274559021\n",
      "2.2654571533203125\n",
      "step 680: train loss 2.2655, val loss 2.2954\n",
      "2.265371799468994\n",
      "2.2652862071990967\n",
      "2.2652013301849365\n",
      "2.2651166915893555\n",
      "2.2650320529937744\n",
      "2.2649476528167725\n",
      "2.2648637294769287\n",
      "2.264779806137085\n",
      "2.2646961212158203\n",
      "2.264612913131714\n",
      "step 690: train loss 2.2646, val loss 2.2947\n",
      "2.264529228210449\n",
      "2.264446258544922\n",
      "2.2643635272979736\n",
      "2.2642810344696045\n",
      "2.2641987800598145\n",
      "2.2641165256500244\n",
      "2.2640345096588135\n",
      "2.2639529705047607\n",
      "2.263871192932129\n",
      "2.2637901306152344\n",
      "step 700: train loss 2.2638, val loss 2.2939\n",
      "2.2637088298797607\n",
      "2.2636282444000244\n",
      "2.263547420501709\n",
      "2.263467311859131\n",
      "2.2633867263793945\n",
      "2.2633068561553955\n",
      "2.2632269859313965\n",
      "2.2631475925445557\n",
      "2.263068199157715\n",
      "2.262989044189453\n",
      "step 710: train loss 2.2630, val loss 2.2932\n",
      "2.2629103660583496\n",
      "2.262830972671509\n",
      "2.2627527713775635\n",
      "2.26267409324646\n",
      "2.262596368789673\n",
      "2.2625181674957275\n",
      "2.2624402046203613\n",
      "2.2623627185821533\n",
      "2.2622852325439453\n",
      "2.2622082233428955\n",
      "step 720: train loss 2.2622, val loss 2.2925\n",
      "2.262131452560425\n",
      "2.262054681777954\n",
      "2.2619779109954834\n",
      "2.261901617050171\n",
      "2.2618255615234375\n",
      "2.261749505996704\n",
      "2.26167368888855\n",
      "2.2615978717803955\n",
      "2.2615225315093994\n",
      "2.2614474296569824\n",
      "step 730: train loss 2.2614, val loss 2.2919\n",
      "2.2613723278045654\n",
      "2.2612974643707275\n",
      "2.2612226009368896\n",
      "2.26114821434021\n",
      "2.2610743045806885\n",
      "2.2609996795654297\n",
      "2.260925769805908\n",
      "2.260852098464966\n",
      "2.2607786655426025\n",
      "2.2607052326202393\n",
      "step 740: train loss 2.2607, val loss 2.2912\n",
      "2.260632276535034\n",
      "2.26055908203125\n",
      "2.260486364364624\n",
      "2.260413646697998\n",
      "2.260341167449951\n",
      "2.2602689266204834\n",
      "2.2601969242095947\n",
      "2.260124683380127\n",
      "2.2600529193878174\n",
      "2.259981393814087\n",
      "step 750: train loss 2.2600, val loss 2.2906\n",
      "2.2599101066589355\n",
      "2.259838819503784\n",
      "2.259767770767212\n",
      "2.2596969604492188\n",
      "2.259626626968384\n",
      "2.2595555782318115\n",
      "2.2594854831695557\n",
      "2.2594151496887207\n",
      "2.259345054626465\n",
      "2.259275436401367\n",
      "step 760: train loss 2.2593, val loss 2.2900\n",
      "2.2592060565948486\n",
      "2.259136199951172\n",
      "2.2590668201446533\n",
      "2.258997678756714\n",
      "2.2589285373687744\n",
      "2.2588601112365723\n",
      "2.258791208267212\n",
      "2.2587227821350098\n",
      "2.2586543560028076\n",
      "2.2585861682891846\n",
      "step 770: train loss 2.2586, val loss 2.2894\n",
      "2.2585179805755615\n",
      "2.2584502696990967\n",
      "2.258382797241211\n",
      "2.258314847946167\n",
      "2.2582478523254395\n",
      "2.2581803798675537\n",
      "2.258113384246826\n",
      "2.258046865463257\n",
      "2.2579798698425293\n",
      "2.25791335105896\n",
      "step 780: train loss 2.2579, val loss 2.2888\n",
      "2.2578468322753906\n",
      "2.2577803134918213\n",
      "2.2577145099639893\n",
      "2.2576487064361572\n",
      "2.257582902908325\n",
      "2.257517099380493\n",
      "2.2574515342712402\n",
      "2.2573866844177246\n",
      "2.2573211193084717\n",
      "2.257256507873535\n",
      "step 790: train loss 2.2573, val loss 2.2882\n",
      "2.2571916580200195\n",
      "2.257126808166504\n",
      "2.2570624351501465\n",
      "2.25699782371521\n",
      "2.2569336891174316\n",
      "2.256869316101074\n",
      "2.256805658340454\n",
      "2.256741762161255\n",
      "2.2566781044006348\n",
      "Reg strength: 0.001, Train loss: 2.2567, Val loss: 2.2882\n",
      "3.748319149017334\n",
      "step 0: train loss 3.7483, val loss 3.7543\n",
      "3.6344263553619385\n",
      "3.550145387649536\n",
      "3.484727382659912\n",
      "3.429079294204712\n",
      "3.3790476322174072\n",
      "3.3334155082702637\n",
      "3.2915987968444824\n",
      "3.253203868865967\n",
      "3.2179064750671387\n",
      "3.185405731201172\n",
      "step 10: train loss 3.1854, val loss 3.1927\n",
      "3.155413866043091\n",
      "3.1276533603668213\n",
      "3.1018667221069336\n",
      "3.0778205394744873\n",
      "3.0553109645843506\n",
      "3.034165382385254\n",
      "3.014239549636841\n",
      "2.9954140186309814\n",
      "2.9775891304016113\n",
      "2.960679054260254\n",
      "step 20: train loss 2.9607, val loss 2.9687\n",
      "2.944612503051758\n",
      "2.929326057434082\n",
      "2.9147629737854004\n",
      "2.9008727073669434\n",
      "2.8876099586486816\n",
      "2.8749325275421143\n",
      "2.862802743911743\n",
      "2.851184844970703\n",
      "2.8400468826293945\n",
      "2.8293585777282715\n",
      "step 30: train loss 2.8294, val loss 2.8381\n",
      "2.819092273712158\n",
      "2.8092219829559326\n",
      "2.7997238636016846\n",
      "2.7905750274658203\n",
      "2.781755208969116\n",
      "2.7732458114624023\n",
      "2.7650275230407715\n",
      "2.757084846496582\n",
      "2.7494020462036133\n",
      "2.741964340209961\n",
      "step 40: train loss 2.7420, val loss 2.7521\n",
      "2.7347593307495117\n",
      "2.7277746200561523\n",
      "2.7209982872009277\n",
      "2.7144196033477783\n",
      "2.708029270172119\n",
      "2.7018179893493652\n",
      "2.6957767009735107\n",
      "2.6898982524871826\n",
      "2.6841742992401123\n",
      "2.678598642349243\n",
      "step 50: train loss 2.6786, val loss 2.6902\n",
      "2.6731646060943604\n",
      "2.6678664684295654\n",
      "2.6626980304718018\n",
      "2.65765380859375\n",
      "2.6527297496795654\n",
      "2.6479198932647705\n",
      "2.643221139907837\n",
      "2.6386282444000244\n",
      "2.6341378688812256\n",
      "2.6297459602355957\n",
      "step 60: train loss 2.6297, val loss 2.6427\n",
      "2.6254496574401855\n",
      "2.6212451457977295\n",
      "2.61712908744812\n",
      "2.6130993366241455\n",
      "2.609152317047119\n",
      "2.605285882949829\n",
      "2.601496934890747\n",
      "2.597783327102661\n",
      "2.5941429138183594\n",
      "2.5905730724334717\n",
      "step 70: train loss 2.5906, val loss 2.6046\n",
      "2.5870726108551025\n",
      "2.5836384296417236\n",
      "2.5802688598632812\n",
      "2.576962471008301\n",
      "2.5737175941467285\n",
      "2.5705316066741943\n",
      "2.567403554916382\n",
      "2.5643320083618164\n",
      "2.5613152980804443\n",
      "2.5583510398864746\n",
      "step 80: train loss 2.5584, val loss 2.5733\n",
      "2.5554392337799072\n",
      "2.5525779724121094\n",
      "2.5497658252716064\n",
      "2.547001361846924\n",
      "2.544283390045166\n",
      "2.541611433029175\n",
      "2.5389838218688965\n",
      "2.5363991260528564\n",
      "2.5338566303253174\n",
      "2.5313549041748047\n",
      "step 90: train loss 2.5314, val loss 2.5471\n",
      "2.52889347076416\n",
      "2.526470899581909\n",
      "2.5240867137908936\n",
      "2.5217397212982178\n",
      "2.5194287300109863\n",
      "2.517153263092041\n",
      "2.5149123668670654\n",
      "2.5127053260803223\n",
      "2.510530948638916\n",
      "2.5083887577056885\n",
      "step 100: train loss 2.5084, val loss 2.5249\n",
      "2.5062780380249023\n",
      "2.5041980743408203\n",
      "2.502147912979126\n",
      "2.5001273155212402\n",
      "2.4981348514556885\n",
      "2.49617075920105\n",
      "2.4942336082458496\n",
      "2.492323637008667\n",
      "2.4904396533966064\n",
      "2.4885809421539307\n",
      "step 110: train loss 2.4886, val loss 2.5057\n",
      "2.4867477416992188\n",
      "2.484938621520996\n",
      "2.483153820037842\n",
      "2.4813923835754395\n",
      "2.4796535968780518\n",
      "2.477937698364258\n",
      "2.476243019104004\n",
      "2.4745705127716064\n",
      "2.472919225692749\n",
      "2.4712882041931152\n",
      "step 120: train loss 2.4713, val loss 2.4891\n",
      "2.469677448272705\n",
      "2.4680871963500977\n",
      "2.46651554107666\n",
      "2.464963674545288\n",
      "2.4634299278259277\n",
      "2.4619152545928955\n",
      "2.460418462753296\n",
      "2.4589388370513916\n",
      "2.45747709274292\n",
      "2.4560320377349854\n",
      "step 130: train loss 2.4560, val loss 2.4744\n",
      "2.454603910446167\n",
      "2.4531919956207275\n",
      "2.451796531677246\n",
      "2.4504165649414062\n",
      "2.449052572250366\n",
      "2.4477038383483887\n",
      "2.4463696479797363\n",
      "2.4450504779815674\n",
      "2.4437460899353027\n",
      "2.442455768585205\n",
      "step 140: train loss 2.4425, val loss 2.4613\n",
      "2.441179037094116\n",
      "2.4399168491363525\n",
      "2.4386675357818604\n",
      "2.437432050704956\n",
      "2.4362096786499023\n",
      "2.434999942779541\n",
      "2.433803081512451\n",
      "2.4326186180114746\n",
      "2.4314465522766113\n",
      "2.430286407470703\n",
      "step 150: train loss 2.4303, val loss 2.4497\n",
      "2.429138660430908\n",
      "2.42800235748291\n",
      "2.42687726020813\n",
      "2.4257640838623047\n",
      "2.424661636352539\n",
      "2.4235706329345703\n",
      "2.422490119934082\n",
      "2.4214208126068115\n",
      "2.4203615188598633\n",
      "2.4193129539489746\n",
      "step 160: train loss 2.4193, val loss 2.4392\n",
      "2.4182746410369873\n",
      "2.417246103286743\n",
      "2.4162278175354004\n",
      "2.415219306945801\n",
      "2.4142203330993652\n",
      "2.413231134414673\n",
      "2.4122509956359863\n",
      "2.411280393600464\n",
      "2.4103188514709473\n",
      "2.4093658924102783\n",
      "step 170: train loss 2.4094, val loss 2.4297\n",
      "2.4084219932556152\n",
      "2.407486915588379\n",
      "2.4065608978271484\n",
      "2.4056429862976074\n",
      "2.404733180999756\n",
      "2.403832197189331\n",
      "2.4029390811920166\n",
      "2.4020540714263916\n",
      "2.401176929473877\n",
      "2.4003076553344727\n",
      "step 180: train loss 2.4003, val loss 2.4211\n",
      "2.3994462490081787\n",
      "2.398592233657837\n",
      "2.3977463245391846\n",
      "2.396907329559326\n",
      "2.39607572555542\n",
      "2.395251750946045\n",
      "2.394434690475464\n",
      "2.3936245441436768\n",
      "2.3928213119506836\n",
      "2.3920249938964844\n",
      "step 190: train loss 2.3920, val loss 2.4132\n",
      "2.391235589981079\n",
      "2.3904528617858887\n",
      "2.389677047729492\n",
      "2.3889071941375732\n",
      "2.388144016265869\n",
      "2.38738751411438\n",
      "2.3866372108459473\n",
      "2.385892629623413\n",
      "2.3851547241210938\n",
      "2.384422540664673\n",
      "step 200: train loss 2.3844, val loss 2.4060\n",
      "2.3836965560913086\n",
      "2.382976531982422\n",
      "2.3822622299194336\n",
      "2.3815536499023438\n",
      "2.3808507919311523\n",
      "2.3801538944244385\n",
      "2.379462242126465\n",
      "2.3787760734558105\n",
      "2.3780951499938965\n",
      "2.37742018699646\n",
      "step 210: train loss 2.3774, val loss 2.3993\n",
      "2.3767499923706055\n",
      "2.3760855197906494\n",
      "2.3754255771636963\n",
      "2.3747713565826416\n",
      "2.374121904373169\n",
      "2.3734774589538574\n",
      "2.372838020324707\n",
      "2.3722035884857178\n",
      "2.3715741634368896\n",
      "2.3709487915039062\n",
      "step 220: train loss 2.3709, val loss 2.3932\n",
      "2.370328903198242\n",
      "2.369713306427002\n",
      "2.3691024780273438\n",
      "2.3684959411621094\n",
      "2.367893934249878\n",
      "2.3672964572906494\n",
      "2.366703748703003\n",
      "2.366114616394043\n",
      "2.365530490875244\n",
      "2.364950180053711\n",
      "step 230: train loss 2.3650, val loss 2.3875\n",
      "2.3643743991851807\n",
      "2.363802671432495\n",
      "2.3632349967956543\n",
      "2.3626718521118164\n",
      "2.362112045288086\n",
      "2.3615562915802\n",
      "2.3610050678253174\n",
      "2.360457181930542\n",
      "2.3599138259887695\n",
      "2.3593735694885254\n",
      "step 240: train loss 2.3594, val loss 2.3823\n",
      "2.358837604522705\n",
      "2.358304977416992\n",
      "2.357776403427124\n",
      "2.3572511672973633\n",
      "2.356729507446289\n",
      "2.3562116622924805\n",
      "2.3556973934173584\n",
      "2.3551864624023438\n",
      "2.3546793460845947\n",
      "2.354175090789795\n",
      "step 250: train loss 2.3542, val loss 2.3774\n",
      "2.3536744117736816\n",
      "2.353177547454834\n",
      "2.3526835441589355\n",
      "2.3521928787231445\n",
      "2.351705551147461\n",
      "2.3512213230133057\n",
      "2.350740671157837\n",
      "2.3502628803253174\n",
      "2.349788188934326\n",
      "2.3493165969848633\n",
      "step 260: train loss 2.3493, val loss 2.3728\n",
      "2.348848342895508\n",
      "2.3483827114105225\n",
      "2.3479204177856445\n",
      "2.347460985183716\n",
      "2.3470048904418945\n",
      "2.346550703048706\n",
      "2.3461005687713623\n",
      "2.3456525802612305\n",
      "2.345207452774048\n",
      "2.3447651863098145\n",
      "step 270: train loss 2.3448, val loss 2.3685\n",
      "2.3443257808685303\n",
      "2.3438894748687744\n",
      "2.3434555530548096\n",
      "2.3430237770080566\n",
      "2.3425955772399902\n",
      "2.3421695232391357\n",
      "2.3417465686798096\n",
      "2.3413257598876953\n",
      "2.340907573699951\n",
      "2.3404922485351562\n",
      "step 280: train loss 2.3405, val loss 2.3645\n",
      "2.3400790691375732\n",
      "2.3396685123443604\n",
      "2.3392605781555176\n",
      "2.338855028152466\n",
      "2.338451862335205\n",
      "2.3380510807037354\n",
      "2.3376526832580566\n",
      "2.337257146835327\n",
      "2.3368632793426514\n",
      "2.3364717960357666\n",
      "step 290: train loss 2.3365, val loss 2.3607\n",
      "2.336082935333252\n",
      "2.33569598197937\n",
      "2.3353116512298584\n",
      "2.3349294662475586\n",
      "2.334549903869629\n",
      "2.334171772003174\n",
      "2.3337960243225098\n",
      "2.3334226608276367\n",
      "2.3330512046813965\n",
      "2.3326823711395264\n",
      "step 300: train loss 2.3327, val loss 2.3572\n",
      "2.332314968109131\n",
      "2.3319504261016846\n",
      "2.331587314605713\n",
      "2.3312265872955322\n",
      "2.3308680057525635\n",
      "2.3305110931396484\n",
      "2.3301565647125244\n",
      "2.329803466796875\n",
      "2.3294527530670166\n",
      "2.329103946685791\n",
      "step 310: train loss 2.3291, val loss 2.3538\n",
      "2.3287570476531982\n",
      "2.3284120559692383\n",
      "2.328068733215332\n",
      "2.327727794647217\n",
      "2.3273885250091553\n",
      "2.3270506858825684\n",
      "2.3267149925231934\n",
      "2.3263814449310303\n",
      "2.326049327850342\n",
      "2.325718879699707\n",
      "step 320: train loss 2.3257, val loss 2.3507\n",
      "2.3253908157348633\n",
      "2.325063943862915\n",
      "2.3247389793395996\n",
      "2.324415922164917\n",
      "2.324094295501709\n",
      "2.323774814605713\n",
      "2.3234565258026123\n",
      "2.3231403827667236\n",
      "2.3228254318237305\n",
      "2.322512626647949\n",
      "step 330: train loss 2.3225, val loss 2.3477\n",
      "2.3222012519836426\n",
      "2.3218917846679688\n",
      "2.3215832710266113\n",
      "2.321276903152466\n",
      "2.320971727371216\n",
      "2.3206684589385986\n",
      "2.320366859436035\n",
      "2.320066452026367\n",
      "2.319767475128174\n",
      "2.3194706439971924\n",
      "step 340: train loss 2.3195, val loss 2.3449\n",
      "2.3191750049591064\n",
      "2.318881034851074\n",
      "2.3185882568359375\n",
      "2.3182971477508545\n",
      "2.318007230758667\n",
      "2.317718744277954\n",
      "2.317432403564453\n",
      "2.3171470165252686\n",
      "2.3168630599975586\n",
      "2.3165805339813232\n",
      "step 350: train loss 2.3166, val loss 2.3422\n",
      "2.3162994384765625\n",
      "2.3160197734832764\n",
      "2.3157413005828857\n",
      "2.315464496612549\n",
      "2.3151888847351074\n",
      "2.3149147033691406\n",
      "2.3146421909332275\n",
      "2.3143701553344727\n",
      "2.314100503921509\n",
      "2.313831090927124\n",
      "step 360: train loss 2.3138, val loss 2.3396\n",
      "2.313563823699951\n",
      "2.3132975101470947\n",
      "2.313032627105713\n",
      "2.3127686977386475\n",
      "2.3125061988830566\n",
      "2.3122448921203613\n",
      "2.311985492706299\n",
      "2.3117268085479736\n",
      "2.311469078063965\n",
      "2.3112130165100098\n",
      "step 370: train loss 2.3112, val loss 2.3372\n",
      "2.310957670211792\n",
      "2.310703992843628\n",
      "2.3104512691497803\n",
      "2.3101999759674072\n",
      "2.3099498748779297\n",
      "2.3097007274627686\n",
      "2.309452533721924\n",
      "2.309206008911133\n",
      "2.308960199356079\n",
      "2.3087158203125\n",
      "step 380: train loss 2.3087, val loss 2.3348\n",
      "2.3084723949432373\n",
      "2.308230400085449\n",
      "2.3079893589019775\n",
      "2.3077495098114014\n",
      "2.3075106143951416\n",
      "2.3072726726531982\n",
      "2.3070361614227295\n",
      "2.306800603866577\n",
      "2.306565761566162\n",
      "2.3063321113586426\n",
      "step 390: train loss 2.3063, val loss 2.3326\n",
      "2.3060998916625977\n",
      "2.30586838722229\n",
      "2.305638074874878\n",
      "2.3054089546203613\n",
      "2.305180549621582\n",
      "2.3049533367156982\n",
      "2.304727077484131\n",
      "2.30450177192688\n",
      "2.3042776584625244\n",
      "2.3040544986724854\n",
      "step 400: train loss 2.3041, val loss 2.3305\n",
      "2.3038320541381836\n",
      "2.3036108016967773\n",
      "2.3033907413482666\n",
      "2.303171396255493\n",
      "2.302952527999878\n",
      "2.3027353286743164\n",
      "2.3025190830230713\n",
      "2.3023033142089844\n",
      "2.302088975906372\n",
      "2.301875114440918\n",
      "step 410: train loss 2.3019, val loss 2.3285\n",
      "2.3016622066497803\n",
      "2.301450490951538\n",
      "2.3012397289276123\n",
      "2.301029682159424\n",
      "2.300820827484131\n",
      "2.300612688064575\n",
      "2.300405263900757\n",
      "2.300198793411255\n",
      "2.2999930381774902\n",
      "2.299788475036621\n",
      "step 420: train loss 2.2998, val loss 2.3266\n",
      "2.2995846271514893\n",
      "2.2993812561035156\n",
      "2.2991795539855957\n",
      "2.298978328704834\n",
      "2.2987778186798096\n",
      "2.2985782623291016\n",
      "2.298379421234131\n",
      "2.2981815338134766\n",
      "2.2979843616485596\n",
      "2.297788143157959\n",
      "step 430: train loss 2.2978, val loss 2.3247\n",
      "2.2975926399230957\n",
      "2.2973978519439697\n",
      "2.2972044944763184\n",
      "2.297011137008667\n",
      "2.296818733215332\n",
      "2.2966272830963135\n",
      "2.2964365482330322\n",
      "2.2962465286254883\n",
      "2.2960574626922607\n",
      "2.2958691120147705\n",
      "step 440: train loss 2.2959, val loss 2.3230\n",
      "2.2956814765930176\n",
      "2.295494556427002\n",
      "2.2953083515167236\n",
      "2.2951231002807617\n",
      "2.294938325881958\n",
      "2.29475474357605\n",
      "2.2945716381073\n",
      "2.294389009475708\n",
      "2.2942073345184326\n",
      "2.2940263748168945\n",
      "step 450: train loss 2.2940, val loss 2.3213\n",
      "2.293846368789673\n",
      "2.2936668395996094\n",
      "2.293487787246704\n",
      "2.2933096885681152\n",
      "2.2931320667266846\n",
      "2.2929553985595703\n",
      "2.2927796840667725\n",
      "2.292604446411133\n",
      "2.2924294471740723\n",
      "2.2922556400299072\n",
      "step 460: train loss 2.2923, val loss 2.3196\n",
      "2.2920823097229004\n",
      "2.2919094562530518\n",
      "2.2917380332946777\n",
      "2.2915663719177246\n",
      "2.291395664215088\n",
      "2.2912256717681885\n",
      "2.2910563945770264\n",
      "2.2908878326416016\n",
      "2.290719747543335\n",
      "2.2905526161193848\n",
      "step 470: train loss 2.2906, val loss 2.3181\n",
      "2.2903854846954346\n",
      "2.29021954536438\n",
      "2.290053606033325\n",
      "2.289888858795166\n",
      "2.289724588394165\n",
      "2.2895610332489014\n",
      "2.289398193359375\n",
      "2.2892355918884277\n",
      "2.289073944091797\n",
      "2.288912534713745\n",
      "step 480: train loss 2.2889, val loss 2.3166\n",
      "2.2887520790100098\n",
      "2.2885921001434326\n",
      "2.2884328365325928\n",
      "2.288274049758911\n",
      "2.2881157398223877\n",
      "2.2879581451416016\n",
      "2.2878010272979736\n",
      "2.287644624710083\n",
      "2.2874886989593506\n",
      "2.2873332500457764\n",
      "step 490: train loss 2.2873, val loss 2.3151\n",
      "2.2871785163879395\n",
      "2.2870242595672607\n",
      "2.2868707180023193\n",
      "2.2867178916931152\n",
      "2.2865653038024902\n",
      "2.2864134311676025\n",
      "2.286261796951294\n",
      "2.2861108779907227\n",
      "2.2859609127044678\n",
      "2.285810947418213\n",
      "step 500: train loss 2.2858, val loss 2.3137\n",
      "2.2856616973876953\n",
      "2.285512685775757\n",
      "2.285364866256714\n",
      "2.285217046737671\n",
      "2.2850699424743652\n",
      "2.2849233150482178\n",
      "2.2847771644592285\n",
      "2.2846317291259766\n",
      "2.2844865322113037\n",
      "2.284342050552368\n",
      "step 510: train loss 2.2843, val loss 2.3124\n",
      "2.284198045730591\n",
      "2.2840545177459717\n",
      "2.28391170501709\n",
      "2.283769130706787\n",
      "2.2836267948150635\n",
      "2.2834854125976562\n",
      "2.283344268798828\n",
      "2.2832038402557373\n",
      "2.2830638885498047\n",
      "2.2829244136810303\n",
      "step 520: train loss 2.2829, val loss 2.3111\n",
      "2.282785177230835\n",
      "2.282646656036377\n",
      "2.282508611679077\n",
      "2.2823708057403564\n",
      "2.282233476638794\n",
      "2.2820968627929688\n",
      "2.2819604873657227\n",
      "2.281824827194214\n",
      "2.2816896438598633\n",
      "2.281554698944092\n",
      "step 530: train loss 2.2816, val loss 2.3099\n",
      "2.2814204692840576\n",
      "2.2812862396240234\n",
      "2.2811529636383057\n",
      "2.281020164489746\n",
      "2.2808876037597656\n",
      "2.280755043029785\n",
      "2.280623435974121\n",
      "2.2804923057556152\n",
      "2.2803616523742676\n",
      "2.280231237411499\n",
      "step 540: train loss 2.2802, val loss 2.3087\n",
      "2.2801010608673096\n",
      "2.2799715995788574\n",
      "2.2798426151275635\n",
      "2.2797138690948486\n",
      "2.279585361480713\n",
      "2.2794580459594727\n",
      "2.2793307304382324\n",
      "2.2792036533355713\n",
      "2.2790772914886475\n",
      "2.2789509296417236\n",
      "step 550: train loss 2.2790, val loss 2.3075\n",
      "2.278825283050537\n",
      "2.278700113296509\n",
      "2.2785754203796387\n",
      "2.2784507274627686\n",
      "2.2783267498016357\n",
      "2.278202772140503\n",
      "2.2780797481536865\n",
      "2.277956962585449\n",
      "2.277834415435791\n",
      "2.277712345123291\n",
      "step 560: train loss 2.2777, val loss 2.3064\n",
      "2.27759051322937\n",
      "2.2774691581726074\n",
      "2.277348518371582\n",
      "2.2772278785705566\n",
      "2.2771077156066895\n",
      "2.2769882678985596\n",
      "2.2768685817718506\n",
      "2.276749610900879\n",
      "2.2766308784484863\n",
      "2.27651309967041\n",
      "step 570: train loss 2.2765, val loss 2.3053\n",
      "2.276395082473755\n",
      "2.2762773036956787\n",
      "2.27616024017334\n",
      "2.2760441303253174\n",
      "2.2759275436401367\n",
      "2.2758114337921143\n",
      "2.275695562362671\n",
      "2.2755801677703857\n",
      "2.275465488433838\n",
      "2.275351047515869\n",
      "step 580: train loss 2.2754, val loss 2.3042\n",
      "2.2752370834350586\n",
      "2.275123119354248\n",
      "2.2750096321105957\n",
      "2.2748966217041016\n",
      "2.2747838497161865\n",
      "2.2746713161468506\n",
      "2.2745590209960938\n",
      "2.274447202682495\n",
      "2.2743358612060547\n",
      "2.2742249965667725\n",
      "step 590: train loss 2.2742, val loss 2.3032\n",
      "2.2741141319274902\n",
      "2.274003744125366\n",
      "2.2738935947418213\n",
      "2.2737841606140137\n",
      "2.273674726486206\n",
      "2.2735657691955566\n",
      "2.2734570503234863\n",
      "2.273348569869995\n",
      "2.273240566253662\n",
      "2.273132801055908\n",
      "step 600: train loss 2.2731, val loss 2.3022\n",
      "2.2730252742767334\n",
      "2.272918462753296\n",
      "2.2728116512298584\n",
      "2.272705316543579\n",
      "2.272599220275879\n",
      "2.272493362426758\n",
      "2.272387981414795\n",
      "2.272282600402832\n",
      "2.2721779346466064\n",
      "2.27207350730896\n",
      "step 610: train loss 2.2721, val loss 2.3013\n",
      "2.2719690799713135\n",
      "2.271865129470825\n",
      "2.271761655807495\n",
      "2.271658182144165\n",
      "2.271555185317993\n",
      "2.2714526653289795\n",
      "2.271350383758545\n",
      "2.2712483406066895\n",
      "2.271146297454834\n",
      "2.271044969558716\n",
      "step 620: train loss 2.2710, val loss 2.3004\n",
      "2.2709436416625977\n",
      "2.270843267440796\n",
      "2.270742177963257\n",
      "2.270641803741455\n",
      "2.2705419063568115\n",
      "2.270442247390747\n",
      "2.270343065261841\n",
      "2.2702438831329346\n",
      "2.2701449394226074\n",
      "2.2700464725494385\n",
      "step 630: train loss 2.2700, val loss 2.2995\n",
      "2.2699480056762695\n",
      "2.269850015640259\n",
      "2.2697525024414062\n",
      "2.2696549892425537\n",
      "2.2695579528808594\n",
      "2.269460916519165\n",
      "2.269364356994629\n",
      "2.269268274307251\n",
      "2.269171953201294\n",
      "2.269076347351074\n",
      "step 640: train loss 2.2691, val loss 2.2986\n",
      "2.2689807415008545\n",
      "2.268885374069214\n",
      "2.2687907218933105\n",
      "2.268695831298828\n",
      "2.268601417541504\n",
      "2.2685070037841797\n",
      "2.268413543701172\n",
      "2.268319845199585\n",
      "2.268226385116577\n",
      "2.2681331634521484\n",
      "step 650: train loss 2.2681, val loss 2.2978\n",
      "2.268040895462036\n",
      "2.2679481506347656\n",
      "2.267855644226074\n",
      "2.26776385307312\n",
      "2.267671585083008\n",
      "2.267580032348633\n",
      "2.267488956451416\n",
      "2.267397880554199\n",
      "2.2673072814941406\n",
      "2.267216682434082\n",
      "step 660: train loss 2.2672, val loss 2.2970\n",
      "2.2671263217926025\n",
      "2.267036199569702\n",
      "2.266946315765381\n",
      "2.266857147216797\n",
      "2.266767740249634\n",
      "2.266678810119629\n",
      "2.266589879989624\n",
      "2.2665011882781982\n",
      "2.2664129734039307\n",
      "2.266324758529663\n",
      "step 670: train loss 2.2663, val loss 2.2962\n",
      "2.2662370204925537\n",
      "2.2661492824554443\n",
      "2.266062021255493\n",
      "2.265974998474121\n",
      "2.265887975692749\n",
      "2.265801429748535\n",
      "2.2657148838043213\n",
      "2.2656288146972656\n",
      "2.26554274559021\n",
      "2.2654571533203125\n",
      "step 680: train loss 2.2655, val loss 2.2954\n",
      "2.265371799468994\n",
      "2.2652862071990967\n",
      "2.2652013301849365\n",
      "2.2651166915893555\n",
      "2.2650320529937744\n",
      "2.2649476528167725\n",
      "2.2648637294769287\n",
      "2.264779806137085\n",
      "2.2646961212158203\n",
      "2.264612913131714\n",
      "step 690: train loss 2.2646, val loss 2.2947\n",
      "2.264529228210449\n",
      "2.264446258544922\n",
      "2.2643635272979736\n",
      "2.2642810344696045\n",
      "2.2641987800598145\n",
      "2.2641165256500244\n",
      "2.2640345096588135\n",
      "2.2639529705047607\n",
      "2.263871192932129\n",
      "2.2637901306152344\n",
      "step 700: train loss 2.2638, val loss 2.2939\n",
      "2.2637088298797607\n",
      "2.2636282444000244\n",
      "2.263547420501709\n",
      "2.263467311859131\n",
      "2.2633867263793945\n",
      "2.2633068561553955\n",
      "2.2632269859313965\n",
      "2.2631475925445557\n",
      "2.263068199157715\n",
      "2.262989044189453\n",
      "step 710: train loss 2.2630, val loss 2.2932\n",
      "2.2629103660583496\n",
      "2.262830972671509\n",
      "2.2627527713775635\n",
      "2.26267409324646\n",
      "2.262596368789673\n",
      "2.2625181674957275\n",
      "2.2624402046203613\n",
      "2.2623627185821533\n",
      "2.2622852325439453\n",
      "2.2622082233428955\n",
      "step 720: train loss 2.2622, val loss 2.2925\n",
      "2.262131452560425\n",
      "2.262054681777954\n",
      "2.2619779109954834\n",
      "2.261901617050171\n",
      "2.2618255615234375\n",
      "2.261749505996704\n",
      "2.26167368888855\n",
      "2.2615978717803955\n",
      "2.2615225315093994\n",
      "2.2614474296569824\n",
      "step 730: train loss 2.2614, val loss 2.2919\n",
      "2.2613723278045654\n",
      "2.2612974643707275\n",
      "2.2612226009368896\n",
      "2.26114821434021\n",
      "2.2610743045806885\n",
      "2.2609996795654297\n",
      "2.260925769805908\n",
      "2.260852098464966\n",
      "2.2607786655426025\n",
      "2.2607052326202393\n",
      "step 740: train loss 2.2607, val loss 2.2912\n",
      "2.260632276535034\n",
      "2.26055908203125\n",
      "2.260486364364624\n",
      "2.260413646697998\n",
      "2.260341167449951\n",
      "2.2602689266204834\n",
      "2.2601969242095947\n",
      "2.260124683380127\n",
      "2.2600529193878174\n",
      "2.259981393814087\n",
      "step 750: train loss 2.2600, val loss 2.2906\n",
      "2.2599101066589355\n",
      "2.259838819503784\n",
      "2.259767770767212\n",
      "2.2596969604492188\n",
      "2.259626626968384\n",
      "2.2595555782318115\n",
      "2.2594854831695557\n",
      "2.2594151496887207\n",
      "2.259345054626465\n",
      "2.259275436401367\n",
      "step 760: train loss 2.2593, val loss 2.2900\n",
      "2.2592060565948486\n",
      "2.259136199951172\n",
      "2.2590668201446533\n",
      "2.258997678756714\n",
      "2.2589285373687744\n",
      "2.2588601112365723\n",
      "2.258791208267212\n",
      "2.2587227821350098\n",
      "2.2586543560028076\n",
      "2.2585861682891846\n",
      "step 770: train loss 2.2586, val loss 2.2894\n",
      "2.2585179805755615\n",
      "2.2584502696990967\n",
      "2.258382797241211\n",
      "2.258314847946167\n",
      "2.2582478523254395\n",
      "2.2581803798675537\n",
      "2.258113384246826\n",
      "2.258046865463257\n",
      "2.2579798698425293\n",
      "2.25791335105896\n",
      "step 780: train loss 2.2579, val loss 2.2888\n",
      "2.2578468322753906\n",
      "2.2577803134918213\n",
      "2.2577145099639893\n",
      "2.2576487064361572\n",
      "2.257582902908325\n",
      "2.257517099380493\n",
      "2.2574515342712402\n",
      "2.2573866844177246\n",
      "2.2573211193084717\n",
      "2.257256507873535\n",
      "step 790: train loss 2.2573, val loss 2.2882\n",
      "2.2571916580200195\n",
      "2.257126808166504\n",
      "2.2570624351501465\n",
      "2.25699782371521\n",
      "2.2569336891174316\n",
      "2.256869316101074\n",
      "2.256805658340454\n",
      "2.256741762161255\n",
      "2.2566781044006348\n",
      "Reg strength: 0.100, Train loss: 2.2567, Val loss: 2.2882\n",
      "3.748319149017334\n",
      "step 0: train loss 3.7483, val loss 3.7543\n",
      "3.6344263553619385\n",
      "3.550145387649536\n",
      "3.484727382659912\n",
      "3.429079294204712\n",
      "3.3790476322174072\n",
      "3.3334155082702637\n",
      "3.2915987968444824\n",
      "3.253203868865967\n",
      "3.2179064750671387\n",
      "3.185405731201172\n",
      "step 10: train loss 3.1854, val loss 3.1927\n",
      "3.155413866043091\n",
      "3.1276533603668213\n",
      "3.1018667221069336\n",
      "3.0778205394744873\n",
      "3.0553109645843506\n",
      "3.034165382385254\n",
      "3.014239549636841\n",
      "2.9954140186309814\n",
      "2.9775891304016113\n",
      "2.960679054260254\n",
      "step 20: train loss 2.9607, val loss 2.9687\n",
      "2.944612503051758\n",
      "2.929326057434082\n",
      "2.9147629737854004\n",
      "2.9008727073669434\n",
      "2.8876099586486816\n",
      "2.8749325275421143\n",
      "2.862802743911743\n",
      "2.851184844970703\n",
      "2.8400468826293945\n",
      "2.8293585777282715\n",
      "step 30: train loss 2.8294, val loss 2.8381\n",
      "2.819092273712158\n",
      "2.8092219829559326\n",
      "2.7997238636016846\n",
      "2.7905750274658203\n",
      "2.781755208969116\n",
      "2.7732458114624023\n",
      "2.7650275230407715\n",
      "2.757084846496582\n",
      "2.7494020462036133\n",
      "2.741964340209961\n",
      "step 40: train loss 2.7420, val loss 2.7521\n",
      "2.7347593307495117\n",
      "2.7277746200561523\n",
      "2.7209982872009277\n",
      "2.7144196033477783\n",
      "2.708029270172119\n",
      "2.7018179893493652\n",
      "2.6957767009735107\n",
      "2.6898982524871826\n",
      "2.6841742992401123\n",
      "2.678598642349243\n",
      "step 50: train loss 2.6786, val loss 2.6902\n",
      "2.6731646060943604\n",
      "2.6678664684295654\n",
      "2.6626980304718018\n",
      "2.65765380859375\n",
      "2.6527297496795654\n",
      "2.6479198932647705\n",
      "2.643221139907837\n",
      "2.6386282444000244\n",
      "2.6341378688812256\n",
      "2.6297459602355957\n",
      "step 60: train loss 2.6297, val loss 2.6427\n",
      "2.6254496574401855\n",
      "2.6212451457977295\n",
      "2.61712908744812\n",
      "2.6130993366241455\n",
      "2.609152317047119\n",
      "2.605285882949829\n",
      "2.601496934890747\n",
      "2.597783327102661\n",
      "2.5941429138183594\n",
      "2.5905730724334717\n",
      "step 70: train loss 2.5906, val loss 2.6046\n",
      "2.5870726108551025\n",
      "2.5836384296417236\n",
      "2.5802688598632812\n",
      "2.576962471008301\n",
      "2.5737175941467285\n",
      "2.5705316066741943\n",
      "2.567403554916382\n",
      "2.5643320083618164\n",
      "2.5613152980804443\n",
      "2.5583510398864746\n",
      "step 80: train loss 2.5584, val loss 2.5733\n",
      "2.5554392337799072\n",
      "2.5525779724121094\n",
      "2.5497658252716064\n",
      "2.547001361846924\n",
      "2.544283390045166\n",
      "2.541611433029175\n",
      "2.5389838218688965\n",
      "2.5363991260528564\n",
      "2.5338566303253174\n",
      "2.5313549041748047\n",
      "step 90: train loss 2.5314, val loss 2.5471\n",
      "2.52889347076416\n",
      "2.526470899581909\n",
      "2.5240867137908936\n",
      "2.5217397212982178\n",
      "2.5194287300109863\n",
      "2.517153263092041\n",
      "2.5149123668670654\n",
      "2.5127053260803223\n",
      "2.510530948638916\n",
      "2.5083887577056885\n",
      "step 100: train loss 2.5084, val loss 2.5249\n",
      "2.5062780380249023\n",
      "2.5041980743408203\n",
      "2.502147912979126\n",
      "2.5001273155212402\n",
      "2.4981348514556885\n",
      "2.49617075920105\n",
      "2.4942336082458496\n",
      "2.492323637008667\n",
      "2.4904396533966064\n",
      "2.4885809421539307\n",
      "step 110: train loss 2.4886, val loss 2.5057\n",
      "2.4867477416992188\n",
      "2.484938621520996\n",
      "2.483153820037842\n",
      "2.4813923835754395\n",
      "2.4796535968780518\n",
      "2.477937698364258\n",
      "2.476243019104004\n",
      "2.4745705127716064\n",
      "2.472919225692749\n",
      "2.4712882041931152\n",
      "step 120: train loss 2.4713, val loss 2.4891\n",
      "2.469677448272705\n",
      "2.4680871963500977\n",
      "2.46651554107666\n",
      "2.464963674545288\n",
      "2.4634299278259277\n",
      "2.4619152545928955\n",
      "2.460418462753296\n",
      "2.4589388370513916\n",
      "2.45747709274292\n",
      "2.4560320377349854\n",
      "step 130: train loss 2.4560, val loss 2.4744\n",
      "2.454603910446167\n",
      "2.4531919956207275\n",
      "2.451796531677246\n",
      "2.4504165649414062\n",
      "2.449052572250366\n",
      "2.4477038383483887\n",
      "2.4463696479797363\n",
      "2.4450504779815674\n",
      "2.4437460899353027\n",
      "2.442455768585205\n",
      "step 140: train loss 2.4425, val loss 2.4613\n",
      "2.441179037094116\n",
      "2.4399168491363525\n",
      "2.4386675357818604\n",
      "2.437432050704956\n",
      "2.4362096786499023\n",
      "2.434999942779541\n",
      "2.433803081512451\n",
      "2.4326186180114746\n",
      "2.4314465522766113\n",
      "2.430286407470703\n",
      "step 150: train loss 2.4303, val loss 2.4497\n",
      "2.429138660430908\n",
      "2.42800235748291\n",
      "2.42687726020813\n",
      "2.4257640838623047\n",
      "2.424661636352539\n",
      "2.4235706329345703\n",
      "2.422490119934082\n",
      "2.4214208126068115\n",
      "2.4203615188598633\n",
      "2.4193129539489746\n",
      "step 160: train loss 2.4193, val loss 2.4392\n",
      "2.4182746410369873\n",
      "2.417246103286743\n",
      "2.4162278175354004\n",
      "2.415219306945801\n",
      "2.4142203330993652\n",
      "2.413231134414673\n",
      "2.4122509956359863\n",
      "2.411280393600464\n",
      "2.4103188514709473\n",
      "2.4093658924102783\n",
      "step 170: train loss 2.4094, val loss 2.4297\n",
      "2.4084219932556152\n",
      "2.407486915588379\n",
      "2.4065608978271484\n",
      "2.4056429862976074\n",
      "2.404733180999756\n",
      "2.403832197189331\n",
      "2.4029390811920166\n",
      "2.4020540714263916\n",
      "2.401176929473877\n",
      "2.4003076553344727\n",
      "step 180: train loss 2.4003, val loss 2.4211\n",
      "2.3994462490081787\n",
      "2.398592233657837\n",
      "2.3977463245391846\n",
      "2.396907329559326\n",
      "2.39607572555542\n",
      "2.395251750946045\n",
      "2.394434690475464\n",
      "2.3936245441436768\n",
      "2.3928213119506836\n",
      "2.3920249938964844\n",
      "step 190: train loss 2.3920, val loss 2.4132\n",
      "2.391235589981079\n",
      "2.3904528617858887\n",
      "2.389677047729492\n",
      "2.3889071941375732\n",
      "2.388144016265869\n",
      "2.38738751411438\n",
      "2.3866372108459473\n",
      "2.385892629623413\n",
      "2.3851547241210938\n",
      "2.384422540664673\n",
      "step 200: train loss 2.3844, val loss 2.4060\n",
      "2.3836965560913086\n",
      "2.382976531982422\n",
      "2.3822622299194336\n",
      "2.3815536499023438\n",
      "2.3808507919311523\n",
      "2.3801538944244385\n",
      "2.379462242126465\n",
      "2.3787760734558105\n",
      "2.3780951499938965\n",
      "2.37742018699646\n",
      "step 210: train loss 2.3774, val loss 2.3993\n",
      "2.3767499923706055\n",
      "2.3760855197906494\n",
      "2.3754255771636963\n",
      "2.3747713565826416\n",
      "2.374121904373169\n",
      "2.3734774589538574\n",
      "2.372838020324707\n",
      "2.3722035884857178\n",
      "2.3715741634368896\n",
      "2.3709487915039062\n",
      "step 220: train loss 2.3709, val loss 2.3932\n",
      "2.370328903198242\n",
      "2.369713306427002\n",
      "2.3691024780273438\n",
      "2.3684959411621094\n",
      "2.367893934249878\n",
      "2.3672964572906494\n",
      "2.366703748703003\n",
      "2.366114616394043\n",
      "2.365530490875244\n",
      "2.364950180053711\n",
      "step 230: train loss 2.3650, val loss 2.3875\n",
      "2.3643743991851807\n",
      "2.363802671432495\n",
      "2.3632349967956543\n",
      "2.3626718521118164\n",
      "2.362112045288086\n",
      "2.3615562915802\n",
      "2.3610050678253174\n",
      "2.360457181930542\n",
      "2.3599138259887695\n",
      "2.3593735694885254\n",
      "step 240: train loss 2.3594, val loss 2.3823\n",
      "2.358837604522705\n",
      "2.358304977416992\n",
      "2.357776403427124\n",
      "2.3572511672973633\n",
      "2.356729507446289\n",
      "2.3562116622924805\n",
      "2.3556973934173584\n",
      "2.3551864624023438\n",
      "2.3546793460845947\n",
      "2.354175090789795\n",
      "step 250: train loss 2.3542, val loss 2.3774\n",
      "2.3536744117736816\n",
      "2.353177547454834\n",
      "2.3526835441589355\n",
      "2.3521928787231445\n",
      "2.351705551147461\n",
      "2.3512213230133057\n",
      "2.350740671157837\n",
      "2.3502628803253174\n",
      "2.349788188934326\n",
      "2.3493165969848633\n",
      "step 260: train loss 2.3493, val loss 2.3728\n",
      "2.348848342895508\n",
      "2.3483827114105225\n",
      "2.3479204177856445\n",
      "2.347460985183716\n",
      "2.3470048904418945\n",
      "2.346550703048706\n",
      "2.3461005687713623\n",
      "2.3456525802612305\n",
      "2.345207452774048\n",
      "2.3447651863098145\n",
      "step 270: train loss 2.3448, val loss 2.3685\n",
      "2.3443257808685303\n",
      "2.3438894748687744\n",
      "2.3434555530548096\n",
      "2.3430237770080566\n",
      "2.3425955772399902\n",
      "2.3421695232391357\n",
      "2.3417465686798096\n",
      "2.3413257598876953\n",
      "2.340907573699951\n",
      "2.3404922485351562\n",
      "step 280: train loss 2.3405, val loss 2.3645\n",
      "2.3400790691375732\n",
      "2.3396685123443604\n",
      "2.3392605781555176\n",
      "2.338855028152466\n",
      "2.338451862335205\n",
      "2.3380510807037354\n",
      "2.3376526832580566\n",
      "2.337257146835327\n",
      "2.3368632793426514\n",
      "2.3364717960357666\n",
      "step 290: train loss 2.3365, val loss 2.3607\n",
      "2.336082935333252\n",
      "2.33569598197937\n",
      "2.3353116512298584\n",
      "2.3349294662475586\n",
      "2.334549903869629\n",
      "2.334171772003174\n",
      "2.3337960243225098\n",
      "2.3334226608276367\n",
      "2.3330512046813965\n",
      "2.3326823711395264\n",
      "step 300: train loss 2.3327, val loss 2.3572\n",
      "2.332314968109131\n",
      "2.3319504261016846\n",
      "2.331587314605713\n",
      "2.3312265872955322\n",
      "2.3308680057525635\n",
      "2.3305110931396484\n",
      "2.3301565647125244\n",
      "2.329803466796875\n",
      "2.3294527530670166\n",
      "2.329103946685791\n",
      "step 310: train loss 2.3291, val loss 2.3538\n",
      "2.3287570476531982\n",
      "2.3284120559692383\n",
      "2.328068733215332\n",
      "2.327727794647217\n",
      "2.3273885250091553\n",
      "2.3270506858825684\n",
      "2.3267149925231934\n",
      "2.3263814449310303\n",
      "2.326049327850342\n",
      "2.325718879699707\n",
      "step 320: train loss 2.3257, val loss 2.3507\n",
      "2.3253908157348633\n",
      "2.325063943862915\n",
      "2.3247389793395996\n",
      "2.324415922164917\n",
      "2.324094295501709\n",
      "2.323774814605713\n",
      "2.3234565258026123\n",
      "2.3231403827667236\n",
      "2.3228254318237305\n",
      "2.322512626647949\n",
      "step 330: train loss 2.3225, val loss 2.3477\n",
      "2.3222012519836426\n",
      "2.3218917846679688\n",
      "2.3215832710266113\n",
      "2.321276903152466\n",
      "2.320971727371216\n",
      "2.3206684589385986\n",
      "2.320366859436035\n",
      "2.320066452026367\n",
      "2.319767475128174\n",
      "2.3194706439971924\n",
      "step 340: train loss 2.3195, val loss 2.3449\n",
      "2.3191750049591064\n",
      "2.318881034851074\n",
      "2.3185882568359375\n",
      "2.3182971477508545\n",
      "2.318007230758667\n",
      "2.317718744277954\n",
      "2.317432403564453\n",
      "2.3171470165252686\n",
      "2.3168630599975586\n",
      "2.3165805339813232\n",
      "step 350: train loss 2.3166, val loss 2.3422\n",
      "2.3162994384765625\n",
      "2.3160197734832764\n",
      "2.3157413005828857\n",
      "2.315464496612549\n",
      "2.3151888847351074\n",
      "2.3149147033691406\n",
      "2.3146421909332275\n",
      "2.3143701553344727\n",
      "2.314100503921509\n",
      "2.313831090927124\n",
      "step 360: train loss 2.3138, val loss 2.3396\n",
      "2.313563823699951\n",
      "2.3132975101470947\n",
      "2.313032627105713\n",
      "2.3127686977386475\n",
      "2.3125061988830566\n",
      "2.3122448921203613\n",
      "2.311985492706299\n",
      "2.3117268085479736\n",
      "2.311469078063965\n",
      "2.3112130165100098\n",
      "step 370: train loss 2.3112, val loss 2.3372\n",
      "2.310957670211792\n",
      "2.310703992843628\n",
      "2.3104512691497803\n",
      "2.3101999759674072\n",
      "2.3099498748779297\n",
      "2.3097007274627686\n",
      "2.309452533721924\n",
      "2.309206008911133\n",
      "2.308960199356079\n",
      "2.3087158203125\n",
      "step 380: train loss 2.3087, val loss 2.3348\n",
      "2.3084723949432373\n",
      "2.308230400085449\n",
      "2.3079893589019775\n",
      "2.3077495098114014\n",
      "2.3075106143951416\n",
      "2.3072726726531982\n",
      "2.3070361614227295\n",
      "2.306800603866577\n",
      "2.306565761566162\n",
      "2.3063321113586426\n",
      "step 390: train loss 2.3063, val loss 2.3326\n",
      "2.3060998916625977\n",
      "2.30586838722229\n",
      "2.305638074874878\n",
      "2.3054089546203613\n",
      "2.305180549621582\n",
      "2.3049533367156982\n",
      "2.304727077484131\n",
      "2.30450177192688\n",
      "2.3042776584625244\n",
      "2.3040544986724854\n",
      "step 400: train loss 2.3041, val loss 2.3305\n",
      "2.3038320541381836\n",
      "2.3036108016967773\n",
      "2.3033907413482666\n",
      "2.303171396255493\n",
      "2.302952527999878\n",
      "2.3027353286743164\n",
      "2.3025190830230713\n",
      "2.3023033142089844\n",
      "2.302088975906372\n",
      "2.301875114440918\n",
      "step 410: train loss 2.3019, val loss 2.3285\n",
      "2.3016622066497803\n",
      "2.301450490951538\n",
      "2.3012397289276123\n",
      "2.301029682159424\n",
      "2.300820827484131\n",
      "2.300612688064575\n",
      "2.300405263900757\n",
      "2.300198793411255\n",
      "2.2999930381774902\n",
      "2.299788475036621\n",
      "step 420: train loss 2.2998, val loss 2.3266\n",
      "2.2995846271514893\n",
      "2.2993812561035156\n",
      "2.2991795539855957\n",
      "2.298978328704834\n",
      "2.2987778186798096\n",
      "2.2985782623291016\n",
      "2.298379421234131\n",
      "2.2981815338134766\n",
      "2.2979843616485596\n",
      "2.297788143157959\n",
      "step 430: train loss 2.2978, val loss 2.3247\n",
      "2.2975926399230957\n",
      "2.2973978519439697\n",
      "2.2972044944763184\n",
      "2.297011137008667\n",
      "2.296818733215332\n",
      "2.2966272830963135\n",
      "2.2964365482330322\n",
      "2.2962465286254883\n",
      "2.2960574626922607\n",
      "2.2958691120147705\n",
      "step 440: train loss 2.2959, val loss 2.3230\n",
      "2.2956814765930176\n",
      "2.295494556427002\n",
      "2.2953083515167236\n",
      "2.2951231002807617\n",
      "2.294938325881958\n",
      "2.29475474357605\n",
      "2.2945716381073\n",
      "2.294389009475708\n",
      "2.2942073345184326\n",
      "2.2940263748168945\n",
      "step 450: train loss 2.2940, val loss 2.3213\n",
      "2.293846368789673\n",
      "2.2936668395996094\n",
      "2.293487787246704\n",
      "2.2933096885681152\n",
      "2.2931320667266846\n",
      "2.2929553985595703\n",
      "2.2927796840667725\n",
      "2.292604446411133\n",
      "2.2924294471740723\n",
      "2.2922556400299072\n",
      "step 460: train loss 2.2923, val loss 2.3196\n",
      "2.2920823097229004\n",
      "2.2919094562530518\n",
      "2.2917380332946777\n",
      "2.2915663719177246\n",
      "2.291395664215088\n",
      "2.2912256717681885\n",
      "2.2910563945770264\n",
      "2.2908878326416016\n",
      "2.290719747543335\n",
      "2.2905526161193848\n",
      "step 470: train loss 2.2906, val loss 2.3181\n",
      "2.2903854846954346\n",
      "2.29021954536438\n",
      "2.290053606033325\n",
      "2.289888858795166\n",
      "2.289724588394165\n",
      "2.2895610332489014\n",
      "2.289398193359375\n",
      "2.2892355918884277\n",
      "2.289073944091797\n",
      "2.288912534713745\n",
      "step 480: train loss 2.2889, val loss 2.3166\n",
      "2.2887520790100098\n",
      "2.2885921001434326\n",
      "2.2884328365325928\n",
      "2.288274049758911\n",
      "2.2881157398223877\n",
      "2.2879581451416016\n",
      "2.2878010272979736\n",
      "2.287644624710083\n",
      "2.2874886989593506\n",
      "2.2873332500457764\n",
      "step 490: train loss 2.2873, val loss 2.3151\n",
      "2.2871785163879395\n",
      "2.2870242595672607\n",
      "2.2868707180023193\n",
      "2.2867178916931152\n",
      "2.2865653038024902\n",
      "2.2864134311676025\n",
      "2.286261796951294\n",
      "2.2861108779907227\n",
      "2.2859609127044678\n",
      "2.285810947418213\n",
      "step 500: train loss 2.2858, val loss 2.3137\n",
      "2.2856616973876953\n",
      "2.285512685775757\n",
      "2.285364866256714\n",
      "2.285217046737671\n",
      "2.2850699424743652\n",
      "2.2849233150482178\n",
      "2.2847771644592285\n",
      "2.2846317291259766\n",
      "2.2844865322113037\n",
      "2.284342050552368\n",
      "step 510: train loss 2.2843, val loss 2.3124\n",
      "2.284198045730591\n",
      "2.2840545177459717\n",
      "2.28391170501709\n",
      "2.283769130706787\n",
      "2.2836267948150635\n",
      "2.2834854125976562\n",
      "2.283344268798828\n",
      "2.2832038402557373\n",
      "2.2830638885498047\n",
      "2.2829244136810303\n",
      "step 520: train loss 2.2829, val loss 2.3111\n",
      "2.282785177230835\n",
      "2.282646656036377\n",
      "2.282508611679077\n",
      "2.2823708057403564\n",
      "2.282233476638794\n",
      "2.2820968627929688\n",
      "2.2819604873657227\n",
      "2.281824827194214\n",
      "2.2816896438598633\n",
      "2.281554698944092\n",
      "step 530: train loss 2.2816, val loss 2.3099\n",
      "2.2814204692840576\n",
      "2.2812862396240234\n",
      "2.2811529636383057\n",
      "2.281020164489746\n",
      "2.2808876037597656\n",
      "2.280755043029785\n",
      "2.280623435974121\n",
      "2.2804923057556152\n",
      "2.2803616523742676\n",
      "2.280231237411499\n",
      "step 540: train loss 2.2802, val loss 2.3087\n",
      "2.2801010608673096\n",
      "2.2799715995788574\n",
      "2.2798426151275635\n",
      "2.2797138690948486\n",
      "2.279585361480713\n",
      "2.2794580459594727\n",
      "2.2793307304382324\n",
      "2.2792036533355713\n",
      "2.2790772914886475\n",
      "2.2789509296417236\n",
      "step 550: train loss 2.2790, val loss 2.3075\n",
      "2.278825283050537\n",
      "2.278700113296509\n",
      "2.2785754203796387\n",
      "2.2784507274627686\n",
      "2.2783267498016357\n",
      "2.278202772140503\n",
      "2.2780797481536865\n",
      "2.277956962585449\n",
      "2.277834415435791\n",
      "2.277712345123291\n",
      "step 560: train loss 2.2777, val loss 2.3064\n",
      "2.27759051322937\n",
      "2.2774691581726074\n",
      "2.277348518371582\n",
      "2.2772278785705566\n",
      "2.2771077156066895\n",
      "2.2769882678985596\n",
      "2.2768685817718506\n",
      "2.276749610900879\n",
      "2.2766308784484863\n",
      "2.27651309967041\n",
      "step 570: train loss 2.2765, val loss 2.3053\n",
      "2.276395082473755\n",
      "2.2762773036956787\n",
      "2.27616024017334\n",
      "2.2760441303253174\n",
      "2.2759275436401367\n",
      "2.2758114337921143\n",
      "2.275695562362671\n",
      "2.2755801677703857\n",
      "2.275465488433838\n",
      "2.275351047515869\n",
      "step 580: train loss 2.2754, val loss 2.3042\n",
      "2.2752370834350586\n",
      "2.275123119354248\n",
      "2.2750096321105957\n",
      "2.2748966217041016\n",
      "2.2747838497161865\n",
      "2.2746713161468506\n",
      "2.2745590209960938\n",
      "2.274447202682495\n",
      "2.2743358612060547\n",
      "2.2742249965667725\n",
      "step 590: train loss 2.2742, val loss 2.3032\n",
      "2.2741141319274902\n",
      "2.274003744125366\n",
      "2.2738935947418213\n",
      "2.2737841606140137\n",
      "2.273674726486206\n",
      "2.2735657691955566\n",
      "2.2734570503234863\n",
      "2.273348569869995\n",
      "2.273240566253662\n",
      "2.273132801055908\n",
      "step 600: train loss 2.2731, val loss 2.3022\n",
      "2.2730252742767334\n",
      "2.272918462753296\n",
      "2.2728116512298584\n",
      "2.272705316543579\n",
      "2.272599220275879\n",
      "2.272493362426758\n",
      "2.272387981414795\n",
      "2.272282600402832\n",
      "2.2721779346466064\n",
      "2.27207350730896\n",
      "step 610: train loss 2.2721, val loss 2.3013\n",
      "2.2719690799713135\n",
      "2.271865129470825\n",
      "2.271761655807495\n",
      "2.271658182144165\n",
      "2.271555185317993\n",
      "2.2714526653289795\n",
      "2.271350383758545\n",
      "2.2712483406066895\n",
      "2.271146297454834\n",
      "2.271044969558716\n",
      "step 620: train loss 2.2710, val loss 2.3004\n",
      "2.2709436416625977\n",
      "2.270843267440796\n",
      "2.270742177963257\n",
      "2.270641803741455\n",
      "2.2705419063568115\n",
      "2.270442247390747\n",
      "2.270343065261841\n",
      "2.2702438831329346\n",
      "2.2701449394226074\n",
      "2.2700464725494385\n",
      "step 630: train loss 2.2700, val loss 2.2995\n",
      "2.2699480056762695\n",
      "2.269850015640259\n",
      "2.2697525024414062\n",
      "2.2696549892425537\n",
      "2.2695579528808594\n",
      "2.269460916519165\n",
      "2.269364356994629\n",
      "2.269268274307251\n",
      "2.269171953201294\n",
      "2.269076347351074\n",
      "step 640: train loss 2.2691, val loss 2.2986\n",
      "2.2689807415008545\n",
      "2.268885374069214\n",
      "2.2687907218933105\n",
      "2.268695831298828\n",
      "2.268601417541504\n",
      "2.2685070037841797\n",
      "2.268413543701172\n",
      "2.268319845199585\n",
      "2.268226385116577\n",
      "2.2681331634521484\n",
      "step 650: train loss 2.2681, val loss 2.2978\n",
      "2.268040895462036\n",
      "2.2679481506347656\n",
      "2.267855644226074\n",
      "2.26776385307312\n",
      "2.267671585083008\n",
      "2.267580032348633\n",
      "2.267488956451416\n",
      "2.267397880554199\n",
      "2.2673072814941406\n",
      "2.267216682434082\n",
      "step 660: train loss 2.2672, val loss 2.2970\n",
      "2.2671263217926025\n",
      "2.267036199569702\n",
      "2.266946315765381\n",
      "2.266857147216797\n",
      "2.266767740249634\n",
      "2.266678810119629\n",
      "2.266589879989624\n",
      "2.2665011882781982\n",
      "2.2664129734039307\n",
      "2.266324758529663\n",
      "step 670: train loss 2.2663, val loss 2.2962\n",
      "2.2662370204925537\n",
      "2.2661492824554443\n",
      "2.266062021255493\n",
      "2.265974998474121\n",
      "2.265887975692749\n",
      "2.265801429748535\n",
      "2.2657148838043213\n",
      "2.2656288146972656\n",
      "2.26554274559021\n",
      "2.2654571533203125\n",
      "step 680: train loss 2.2655, val loss 2.2954\n",
      "2.265371799468994\n",
      "2.2652862071990967\n",
      "2.2652013301849365\n",
      "2.2651166915893555\n",
      "2.2650320529937744\n",
      "2.2649476528167725\n",
      "2.2648637294769287\n",
      "2.264779806137085\n",
      "2.2646961212158203\n",
      "2.264612913131714\n",
      "step 690: train loss 2.2646, val loss 2.2947\n",
      "2.264529228210449\n",
      "2.264446258544922\n",
      "2.2643635272979736\n",
      "2.2642810344696045\n",
      "2.2641987800598145\n",
      "2.2641165256500244\n",
      "2.2640345096588135\n",
      "2.2639529705047607\n",
      "2.263871192932129\n",
      "2.2637901306152344\n",
      "step 700: train loss 2.2638, val loss 2.2939\n",
      "2.2637088298797607\n",
      "2.2636282444000244\n",
      "2.263547420501709\n",
      "2.263467311859131\n",
      "2.2633867263793945\n",
      "2.2633068561553955\n",
      "2.2632269859313965\n",
      "2.2631475925445557\n",
      "2.263068199157715\n",
      "2.262989044189453\n",
      "step 710: train loss 2.2630, val loss 2.2932\n",
      "2.2629103660583496\n",
      "2.262830972671509\n",
      "2.2627527713775635\n",
      "2.26267409324646\n",
      "2.262596368789673\n",
      "2.2625181674957275\n",
      "2.2624402046203613\n",
      "2.2623627185821533\n",
      "2.2622852325439453\n",
      "2.2622082233428955\n",
      "step 720: train loss 2.2622, val loss 2.2925\n",
      "2.262131452560425\n",
      "2.262054681777954\n",
      "2.2619779109954834\n",
      "2.261901617050171\n",
      "2.2618255615234375\n",
      "2.261749505996704\n",
      "2.26167368888855\n",
      "2.2615978717803955\n",
      "2.2615225315093994\n",
      "2.2614474296569824\n",
      "step 730: train loss 2.2614, val loss 2.2919\n",
      "2.2613723278045654\n",
      "2.2612974643707275\n",
      "2.2612226009368896\n",
      "2.26114821434021\n",
      "2.2610743045806885\n",
      "2.2609996795654297\n",
      "2.260925769805908\n",
      "2.260852098464966\n",
      "2.2607786655426025\n",
      "2.2607052326202393\n",
      "step 740: train loss 2.2607, val loss 2.2912\n",
      "2.260632276535034\n",
      "2.26055908203125\n",
      "2.260486364364624\n",
      "2.260413646697998\n",
      "2.260341167449951\n",
      "2.2602689266204834\n",
      "2.2601969242095947\n",
      "2.260124683380127\n",
      "2.2600529193878174\n",
      "2.259981393814087\n",
      "step 750: train loss 2.2600, val loss 2.2906\n",
      "2.2599101066589355\n",
      "2.259838819503784\n",
      "2.259767770767212\n",
      "2.2596969604492188\n",
      "2.259626626968384\n",
      "2.2595555782318115\n",
      "2.2594854831695557\n",
      "2.2594151496887207\n",
      "2.259345054626465\n",
      "2.259275436401367\n",
      "step 760: train loss 2.2593, val loss 2.2900\n",
      "2.2592060565948486\n",
      "2.259136199951172\n",
      "2.2590668201446533\n",
      "2.258997678756714\n",
      "2.2589285373687744\n",
      "2.2588601112365723\n",
      "2.258791208267212\n",
      "2.2587227821350098\n",
      "2.2586543560028076\n",
      "2.2585861682891846\n",
      "step 770: train loss 2.2586, val loss 2.2894\n",
      "2.2585179805755615\n",
      "2.2584502696990967\n",
      "2.258382797241211\n",
      "2.258314847946167\n",
      "2.2582478523254395\n",
      "2.2581803798675537\n",
      "2.258113384246826\n",
      "2.258046865463257\n",
      "2.2579798698425293\n",
      "2.25791335105896\n",
      "step 780: train loss 2.2579, val loss 2.2888\n",
      "2.2578468322753906\n",
      "2.2577803134918213\n",
      "2.2577145099639893\n",
      "2.2576487064361572\n",
      "2.257582902908325\n",
      "2.257517099380493\n",
      "2.2574515342712402\n",
      "2.2573866844177246\n",
      "2.2573211193084717\n",
      "2.257256507873535\n",
      "step 790: train loss 2.2573, val loss 2.2882\n",
      "2.2571916580200195\n",
      "2.257126808166504\n",
      "2.2570624351501465\n",
      "2.25699782371521\n",
      "2.2569336891174316\n",
      "2.256869316101074\n",
      "2.256805658340454\n",
      "2.256741762161255\n",
      "2.2566781044006348\n",
      "Reg strength: 0.500, Train loss: 2.2567, Val loss: 2.2882\n",
      "3.748319149017334\n",
      "step 0: train loss 3.7483, val loss 3.7543\n",
      "3.6344263553619385\n",
      "3.550145387649536\n",
      "3.484727382659912\n",
      "3.429079294204712\n",
      "3.3790476322174072\n",
      "3.3334155082702637\n",
      "3.2915987968444824\n",
      "3.253203868865967\n",
      "3.2179064750671387\n",
      "3.185405731201172\n",
      "step 10: train loss 3.1854, val loss 3.1927\n",
      "3.155413866043091\n",
      "3.1276533603668213\n",
      "3.1018667221069336\n",
      "3.0778205394744873\n",
      "3.0553109645843506\n",
      "3.034165382385254\n",
      "3.014239549636841\n",
      "2.9954140186309814\n",
      "2.9775891304016113\n",
      "2.960679054260254\n",
      "step 20: train loss 2.9607, val loss 2.9687\n",
      "2.944612503051758\n",
      "2.929326057434082\n",
      "2.9147629737854004\n",
      "2.9008727073669434\n",
      "2.8876099586486816\n",
      "2.8749325275421143\n",
      "2.862802743911743\n",
      "2.851184844970703\n",
      "2.8400468826293945\n",
      "2.8293585777282715\n",
      "step 30: train loss 2.8294, val loss 2.8381\n",
      "2.819092273712158\n",
      "2.8092219829559326\n",
      "2.7997238636016846\n",
      "2.7905750274658203\n",
      "2.781755208969116\n",
      "2.7732458114624023\n",
      "2.7650275230407715\n",
      "2.757084846496582\n",
      "2.7494020462036133\n",
      "2.741964340209961\n",
      "step 40: train loss 2.7420, val loss 2.7521\n",
      "2.7347593307495117\n",
      "2.7277746200561523\n",
      "2.7209982872009277\n",
      "2.7144196033477783\n",
      "2.708029270172119\n",
      "2.7018179893493652\n",
      "2.6957767009735107\n",
      "2.6898982524871826\n",
      "2.6841742992401123\n",
      "2.678598642349243\n",
      "step 50: train loss 2.6786, val loss 2.6902\n",
      "2.6731646060943604\n",
      "2.6678664684295654\n",
      "2.6626980304718018\n",
      "2.65765380859375\n",
      "2.6527297496795654\n",
      "2.6479198932647705\n",
      "2.643221139907837\n",
      "2.6386282444000244\n",
      "2.6341378688812256\n",
      "2.6297459602355957\n",
      "step 60: train loss 2.6297, val loss 2.6427\n",
      "2.6254496574401855\n",
      "2.6212451457977295\n",
      "2.61712908744812\n",
      "2.6130993366241455\n",
      "2.609152317047119\n",
      "2.605285882949829\n",
      "2.601496934890747\n",
      "2.597783327102661\n",
      "2.5941429138183594\n",
      "2.5905730724334717\n",
      "step 70: train loss 2.5906, val loss 2.6046\n",
      "2.5870726108551025\n",
      "2.5836384296417236\n",
      "2.5802688598632812\n",
      "2.576962471008301\n",
      "2.5737175941467285\n",
      "2.5705316066741943\n",
      "2.567403554916382\n",
      "2.5643320083618164\n",
      "2.5613152980804443\n",
      "2.5583510398864746\n",
      "step 80: train loss 2.5584, val loss 2.5733\n",
      "2.5554392337799072\n",
      "2.5525779724121094\n",
      "2.5497658252716064\n",
      "2.547001361846924\n",
      "2.544283390045166\n",
      "2.541611433029175\n",
      "2.5389838218688965\n",
      "2.5363991260528564\n",
      "2.5338566303253174\n",
      "2.5313549041748047\n",
      "step 90: train loss 2.5314, val loss 2.5471\n",
      "2.52889347076416\n",
      "2.526470899581909\n",
      "2.5240867137908936\n",
      "2.5217397212982178\n",
      "2.5194287300109863\n",
      "2.517153263092041\n",
      "2.5149123668670654\n",
      "2.5127053260803223\n",
      "2.510530948638916\n",
      "2.5083887577056885\n",
      "step 100: train loss 2.5084, val loss 2.5249\n",
      "2.5062780380249023\n",
      "2.5041980743408203\n",
      "2.502147912979126\n",
      "2.5001273155212402\n",
      "2.4981348514556885\n",
      "2.49617075920105\n",
      "2.4942336082458496\n",
      "2.492323637008667\n",
      "2.4904396533966064\n",
      "2.4885809421539307\n",
      "step 110: train loss 2.4886, val loss 2.5057\n",
      "2.4867477416992188\n",
      "2.484938621520996\n",
      "2.483153820037842\n",
      "2.4813923835754395\n",
      "2.4796535968780518\n",
      "2.477937698364258\n",
      "2.476243019104004\n",
      "2.4745705127716064\n",
      "2.472919225692749\n",
      "2.4712882041931152\n",
      "step 120: train loss 2.4713, val loss 2.4891\n",
      "2.469677448272705\n",
      "2.4680871963500977\n",
      "2.46651554107666\n",
      "2.464963674545288\n",
      "2.4634299278259277\n",
      "2.4619152545928955\n",
      "2.460418462753296\n",
      "2.4589388370513916\n",
      "2.45747709274292\n",
      "2.4560320377349854\n",
      "step 130: train loss 2.4560, val loss 2.4744\n",
      "2.454603910446167\n",
      "2.4531919956207275\n",
      "2.451796531677246\n",
      "2.4504165649414062\n",
      "2.449052572250366\n",
      "2.4477038383483887\n",
      "2.4463696479797363\n",
      "2.4450504779815674\n",
      "2.4437460899353027\n",
      "2.442455768585205\n",
      "step 140: train loss 2.4425, val loss 2.4613\n",
      "2.441179037094116\n",
      "2.4399168491363525\n",
      "2.4386675357818604\n",
      "2.437432050704956\n",
      "2.4362096786499023\n",
      "2.434999942779541\n",
      "2.433803081512451\n",
      "2.4326186180114746\n",
      "2.4314465522766113\n",
      "2.430286407470703\n",
      "step 150: train loss 2.4303, val loss 2.4497\n",
      "2.429138660430908\n",
      "2.42800235748291\n",
      "2.42687726020813\n",
      "2.4257640838623047\n",
      "2.424661636352539\n",
      "2.4235706329345703\n",
      "2.422490119934082\n",
      "2.4214208126068115\n",
      "2.4203615188598633\n",
      "2.4193129539489746\n",
      "step 160: train loss 2.4193, val loss 2.4392\n",
      "2.4182746410369873\n",
      "2.417246103286743\n",
      "2.4162278175354004\n",
      "2.415219306945801\n",
      "2.4142203330993652\n",
      "2.413231134414673\n",
      "2.4122509956359863\n",
      "2.411280393600464\n",
      "2.4103188514709473\n",
      "2.4093658924102783\n",
      "step 170: train loss 2.4094, val loss 2.4297\n",
      "2.4084219932556152\n",
      "2.407486915588379\n",
      "2.4065608978271484\n",
      "2.4056429862976074\n",
      "2.404733180999756\n",
      "2.403832197189331\n",
      "2.4029390811920166\n",
      "2.4020540714263916\n",
      "2.401176929473877\n",
      "2.4003076553344727\n",
      "step 180: train loss 2.4003, val loss 2.4211\n",
      "2.3994462490081787\n",
      "2.398592233657837\n",
      "2.3977463245391846\n",
      "2.396907329559326\n",
      "2.39607572555542\n",
      "2.395251750946045\n",
      "2.394434690475464\n",
      "2.3936245441436768\n",
      "2.3928213119506836\n",
      "2.3920249938964844\n",
      "step 190: train loss 2.3920, val loss 2.4132\n",
      "2.391235589981079\n",
      "2.3904528617858887\n",
      "2.389677047729492\n",
      "2.3889071941375732\n",
      "2.388144016265869\n",
      "2.38738751411438\n",
      "2.3866372108459473\n",
      "2.385892629623413\n",
      "2.3851547241210938\n",
      "2.384422540664673\n",
      "step 200: train loss 2.3844, val loss 2.4060\n",
      "2.3836965560913086\n",
      "2.382976531982422\n",
      "2.3822622299194336\n",
      "2.3815536499023438\n",
      "2.3808507919311523\n",
      "2.3801538944244385\n",
      "2.379462242126465\n",
      "2.3787760734558105\n",
      "2.3780951499938965\n",
      "2.37742018699646\n",
      "step 210: train loss 2.3774, val loss 2.3993\n",
      "2.3767499923706055\n",
      "2.3760855197906494\n",
      "2.3754255771636963\n",
      "2.3747713565826416\n",
      "2.374121904373169\n",
      "2.3734774589538574\n",
      "2.372838020324707\n",
      "2.3722035884857178\n",
      "2.3715741634368896\n",
      "2.3709487915039062\n",
      "step 220: train loss 2.3709, val loss 2.3932\n",
      "2.370328903198242\n",
      "2.369713306427002\n",
      "2.3691024780273438\n",
      "2.3684959411621094\n",
      "2.367893934249878\n",
      "2.3672964572906494\n",
      "2.366703748703003\n",
      "2.366114616394043\n",
      "2.365530490875244\n",
      "2.364950180053711\n",
      "step 230: train loss 2.3650, val loss 2.3875\n",
      "2.3643743991851807\n",
      "2.363802671432495\n",
      "2.3632349967956543\n",
      "2.3626718521118164\n",
      "2.362112045288086\n",
      "2.3615562915802\n",
      "2.3610050678253174\n",
      "2.360457181930542\n",
      "2.3599138259887695\n",
      "2.3593735694885254\n",
      "step 240: train loss 2.3594, val loss 2.3823\n",
      "2.358837604522705\n",
      "2.358304977416992\n",
      "2.357776403427124\n",
      "2.3572511672973633\n",
      "2.356729507446289\n",
      "2.3562116622924805\n",
      "2.3556973934173584\n",
      "2.3551864624023438\n",
      "2.3546793460845947\n",
      "2.354175090789795\n",
      "step 250: train loss 2.3542, val loss 2.3774\n",
      "2.3536744117736816\n",
      "2.353177547454834\n",
      "2.3526835441589355\n",
      "2.3521928787231445\n",
      "2.351705551147461\n",
      "2.3512213230133057\n",
      "2.350740671157837\n",
      "2.3502628803253174\n",
      "2.349788188934326\n",
      "2.3493165969848633\n",
      "step 260: train loss 2.3493, val loss 2.3728\n",
      "2.348848342895508\n",
      "2.3483827114105225\n",
      "2.3479204177856445\n",
      "2.347460985183716\n",
      "2.3470048904418945\n",
      "2.346550703048706\n",
      "2.3461005687713623\n",
      "2.3456525802612305\n",
      "2.345207452774048\n",
      "2.3447651863098145\n",
      "step 270: train loss 2.3448, val loss 2.3685\n",
      "2.3443257808685303\n",
      "2.3438894748687744\n",
      "2.3434555530548096\n",
      "2.3430237770080566\n",
      "2.3425955772399902\n",
      "2.3421695232391357\n",
      "2.3417465686798096\n",
      "2.3413257598876953\n",
      "2.340907573699951\n",
      "2.3404922485351562\n",
      "step 280: train loss 2.3405, val loss 2.3645\n",
      "2.3400790691375732\n",
      "2.3396685123443604\n",
      "2.3392605781555176\n",
      "2.338855028152466\n",
      "2.338451862335205\n",
      "2.3380510807037354\n",
      "2.3376526832580566\n",
      "2.337257146835327\n",
      "2.3368632793426514\n",
      "2.3364717960357666\n",
      "step 290: train loss 2.3365, val loss 2.3607\n",
      "2.336082935333252\n",
      "2.33569598197937\n",
      "2.3353116512298584\n",
      "2.3349294662475586\n",
      "2.334549903869629\n",
      "2.334171772003174\n",
      "2.3337960243225098\n",
      "2.3334226608276367\n",
      "2.3330512046813965\n",
      "2.3326823711395264\n",
      "step 300: train loss 2.3327, val loss 2.3572\n",
      "2.332314968109131\n",
      "2.3319504261016846\n",
      "2.331587314605713\n",
      "2.3312265872955322\n",
      "2.3308680057525635\n",
      "2.3305110931396484\n",
      "2.3301565647125244\n",
      "2.329803466796875\n",
      "2.3294527530670166\n",
      "2.329103946685791\n",
      "step 310: train loss 2.3291, val loss 2.3538\n",
      "2.3287570476531982\n",
      "2.3284120559692383\n",
      "2.328068733215332\n",
      "2.327727794647217\n",
      "2.3273885250091553\n",
      "2.3270506858825684\n",
      "2.3267149925231934\n",
      "2.3263814449310303\n",
      "2.326049327850342\n",
      "2.325718879699707\n",
      "step 320: train loss 2.3257, val loss 2.3507\n",
      "2.3253908157348633\n",
      "2.325063943862915\n",
      "2.3247389793395996\n",
      "2.324415922164917\n",
      "2.324094295501709\n",
      "2.323774814605713\n",
      "2.3234565258026123\n",
      "2.3231403827667236\n",
      "2.3228254318237305\n",
      "2.322512626647949\n",
      "step 330: train loss 2.3225, val loss 2.3477\n",
      "2.3222012519836426\n",
      "2.3218917846679688\n",
      "2.3215832710266113\n",
      "2.321276903152466\n",
      "2.320971727371216\n",
      "2.3206684589385986\n",
      "2.320366859436035\n",
      "2.320066452026367\n",
      "2.319767475128174\n",
      "2.3194706439971924\n",
      "step 340: train loss 2.3195, val loss 2.3449\n",
      "2.3191750049591064\n",
      "2.318881034851074\n",
      "2.3185882568359375\n",
      "2.3182971477508545\n",
      "2.318007230758667\n",
      "2.317718744277954\n",
      "2.317432403564453\n",
      "2.3171470165252686\n",
      "2.3168630599975586\n",
      "2.3165805339813232\n",
      "step 350: train loss 2.3166, val loss 2.3422\n",
      "2.3162994384765625\n",
      "2.3160197734832764\n",
      "2.3157413005828857\n",
      "2.315464496612549\n",
      "2.3151888847351074\n",
      "2.3149147033691406\n",
      "2.3146421909332275\n",
      "2.3143701553344727\n",
      "2.314100503921509\n",
      "2.313831090927124\n",
      "step 360: train loss 2.3138, val loss 2.3396\n",
      "2.313563823699951\n",
      "2.3132975101470947\n",
      "2.313032627105713\n",
      "2.3127686977386475\n",
      "2.3125061988830566\n",
      "2.3122448921203613\n",
      "2.311985492706299\n",
      "2.3117268085479736\n",
      "2.311469078063965\n",
      "2.3112130165100098\n",
      "step 370: train loss 2.3112, val loss 2.3372\n",
      "2.310957670211792\n",
      "2.310703992843628\n",
      "2.3104512691497803\n",
      "2.3101999759674072\n",
      "2.3099498748779297\n",
      "2.3097007274627686\n",
      "2.309452533721924\n",
      "2.309206008911133\n",
      "2.308960199356079\n",
      "2.3087158203125\n",
      "step 380: train loss 2.3087, val loss 2.3348\n",
      "2.3084723949432373\n",
      "2.308230400085449\n",
      "2.3079893589019775\n",
      "2.3077495098114014\n",
      "2.3075106143951416\n",
      "2.3072726726531982\n",
      "2.3070361614227295\n",
      "2.306800603866577\n",
      "2.306565761566162\n",
      "2.3063321113586426\n",
      "step 390: train loss 2.3063, val loss 2.3326\n",
      "2.3060998916625977\n",
      "2.30586838722229\n",
      "2.305638074874878\n",
      "2.3054089546203613\n",
      "2.305180549621582\n",
      "2.3049533367156982\n",
      "2.304727077484131\n",
      "2.30450177192688\n",
      "2.3042776584625244\n",
      "2.3040544986724854\n",
      "step 400: train loss 2.3041, val loss 2.3305\n",
      "2.3038320541381836\n",
      "2.3036108016967773\n",
      "2.3033907413482666\n",
      "2.303171396255493\n",
      "2.302952527999878\n",
      "2.3027353286743164\n",
      "2.3025190830230713\n",
      "2.3023033142089844\n",
      "2.302088975906372\n",
      "2.301875114440918\n",
      "step 410: train loss 2.3019, val loss 2.3285\n",
      "2.3016622066497803\n",
      "2.301450490951538\n",
      "2.3012397289276123\n",
      "2.301029682159424\n",
      "2.300820827484131\n",
      "2.300612688064575\n",
      "2.300405263900757\n",
      "2.300198793411255\n",
      "2.2999930381774902\n",
      "2.299788475036621\n",
      "step 420: train loss 2.2998, val loss 2.3266\n",
      "2.2995846271514893\n",
      "2.2993812561035156\n",
      "2.2991795539855957\n",
      "2.298978328704834\n",
      "2.2987778186798096\n",
      "2.2985782623291016\n",
      "2.298379421234131\n",
      "2.2981815338134766\n",
      "2.2979843616485596\n",
      "2.297788143157959\n",
      "step 430: train loss 2.2978, val loss 2.3247\n",
      "2.2975926399230957\n",
      "2.2973978519439697\n",
      "2.2972044944763184\n",
      "2.297011137008667\n",
      "2.296818733215332\n",
      "2.2966272830963135\n",
      "2.2964365482330322\n",
      "2.2962465286254883\n",
      "2.2960574626922607\n",
      "2.2958691120147705\n",
      "step 440: train loss 2.2959, val loss 2.3230\n",
      "2.2956814765930176\n",
      "2.295494556427002\n",
      "2.2953083515167236\n",
      "2.2951231002807617\n",
      "2.294938325881958\n",
      "2.29475474357605\n",
      "2.2945716381073\n",
      "2.294389009475708\n",
      "2.2942073345184326\n",
      "2.2940263748168945\n",
      "step 450: train loss 2.2940, val loss 2.3213\n",
      "2.293846368789673\n",
      "2.2936668395996094\n",
      "2.293487787246704\n",
      "2.2933096885681152\n",
      "2.2931320667266846\n",
      "2.2929553985595703\n",
      "2.2927796840667725\n",
      "2.292604446411133\n",
      "2.2924294471740723\n",
      "2.2922556400299072\n",
      "step 460: train loss 2.2923, val loss 2.3196\n",
      "2.2920823097229004\n",
      "2.2919094562530518\n",
      "2.2917380332946777\n",
      "2.2915663719177246\n",
      "2.291395664215088\n",
      "2.2912256717681885\n",
      "2.2910563945770264\n",
      "2.2908878326416016\n",
      "2.290719747543335\n",
      "2.2905526161193848\n",
      "step 470: train loss 2.2906, val loss 2.3181\n",
      "2.2903854846954346\n",
      "2.29021954536438\n",
      "2.290053606033325\n",
      "2.289888858795166\n",
      "2.289724588394165\n",
      "2.2895610332489014\n",
      "2.289398193359375\n",
      "2.2892355918884277\n",
      "2.289073944091797\n",
      "2.288912534713745\n",
      "step 480: train loss 2.2889, val loss 2.3166\n",
      "2.2887520790100098\n",
      "2.2885921001434326\n",
      "2.2884328365325928\n",
      "2.288274049758911\n",
      "2.2881157398223877\n",
      "2.2879581451416016\n",
      "2.2878010272979736\n",
      "2.287644624710083\n",
      "2.2874886989593506\n",
      "2.2873332500457764\n",
      "step 490: train loss 2.2873, val loss 2.3151\n",
      "2.2871785163879395\n",
      "2.2870242595672607\n",
      "2.2868707180023193\n",
      "2.2867178916931152\n",
      "2.2865653038024902\n",
      "2.2864134311676025\n",
      "2.286261796951294\n",
      "2.2861108779907227\n",
      "2.2859609127044678\n",
      "2.285810947418213\n",
      "step 500: train loss 2.2858, val loss 2.3137\n",
      "2.2856616973876953\n",
      "2.285512685775757\n",
      "2.285364866256714\n",
      "2.285217046737671\n",
      "2.2850699424743652\n",
      "2.2849233150482178\n",
      "2.2847771644592285\n",
      "2.2846317291259766\n",
      "2.2844865322113037\n",
      "2.284342050552368\n",
      "step 510: train loss 2.2843, val loss 2.3124\n",
      "2.284198045730591\n",
      "2.2840545177459717\n",
      "2.28391170501709\n",
      "2.283769130706787\n",
      "2.2836267948150635\n",
      "2.2834854125976562\n",
      "2.283344268798828\n",
      "2.2832038402557373\n",
      "2.2830638885498047\n",
      "2.2829244136810303\n",
      "step 520: train loss 2.2829, val loss 2.3111\n",
      "2.282785177230835\n",
      "2.282646656036377\n",
      "2.282508611679077\n",
      "2.2823708057403564\n",
      "2.282233476638794\n",
      "2.2820968627929688\n",
      "2.2819604873657227\n",
      "2.281824827194214\n",
      "2.2816896438598633\n",
      "2.281554698944092\n",
      "step 530: train loss 2.2816, val loss 2.3099\n",
      "2.2814204692840576\n",
      "2.2812862396240234\n",
      "2.2811529636383057\n",
      "2.281020164489746\n",
      "2.2808876037597656\n",
      "2.280755043029785\n",
      "2.280623435974121\n",
      "2.2804923057556152\n",
      "2.2803616523742676\n",
      "2.280231237411499\n",
      "step 540: train loss 2.2802, val loss 2.3087\n",
      "2.2801010608673096\n",
      "2.2799715995788574\n",
      "2.2798426151275635\n",
      "2.2797138690948486\n",
      "2.279585361480713\n",
      "2.2794580459594727\n",
      "2.2793307304382324\n",
      "2.2792036533355713\n",
      "2.2790772914886475\n",
      "2.2789509296417236\n",
      "step 550: train loss 2.2790, val loss 2.3075\n",
      "2.278825283050537\n",
      "2.278700113296509\n",
      "2.2785754203796387\n",
      "2.2784507274627686\n",
      "2.2783267498016357\n",
      "2.278202772140503\n",
      "2.2780797481536865\n",
      "2.277956962585449\n",
      "2.277834415435791\n",
      "2.277712345123291\n",
      "step 560: train loss 2.2777, val loss 2.3064\n",
      "2.27759051322937\n",
      "2.2774691581726074\n",
      "2.277348518371582\n",
      "2.2772278785705566\n",
      "2.2771077156066895\n",
      "2.2769882678985596\n",
      "2.2768685817718506\n",
      "2.276749610900879\n",
      "2.2766308784484863\n",
      "2.27651309967041\n",
      "step 570: train loss 2.2765, val loss 2.3053\n",
      "2.276395082473755\n",
      "2.2762773036956787\n",
      "2.27616024017334\n",
      "2.2760441303253174\n",
      "2.2759275436401367\n",
      "2.2758114337921143\n",
      "2.275695562362671\n",
      "2.2755801677703857\n",
      "2.275465488433838\n",
      "2.275351047515869\n",
      "step 580: train loss 2.2754, val loss 2.3042\n",
      "2.2752370834350586\n",
      "2.275123119354248\n",
      "2.2750096321105957\n",
      "2.2748966217041016\n",
      "2.2747838497161865\n",
      "2.2746713161468506\n",
      "2.2745590209960938\n",
      "2.274447202682495\n",
      "2.2743358612060547\n",
      "2.2742249965667725\n",
      "step 590: train loss 2.2742, val loss 2.3032\n",
      "2.2741141319274902\n",
      "2.274003744125366\n",
      "2.2738935947418213\n",
      "2.2737841606140137\n",
      "2.273674726486206\n",
      "2.2735657691955566\n",
      "2.2734570503234863\n",
      "2.273348569869995\n",
      "2.273240566253662\n",
      "2.273132801055908\n",
      "step 600: train loss 2.2731, val loss 2.3022\n",
      "2.2730252742767334\n",
      "2.272918462753296\n",
      "2.2728116512298584\n",
      "2.272705316543579\n",
      "2.272599220275879\n",
      "2.272493362426758\n",
      "2.272387981414795\n",
      "2.272282600402832\n",
      "2.2721779346466064\n",
      "2.27207350730896\n",
      "step 610: train loss 2.2721, val loss 2.3013\n",
      "2.2719690799713135\n",
      "2.271865129470825\n",
      "2.271761655807495\n",
      "2.271658182144165\n",
      "2.271555185317993\n",
      "2.2714526653289795\n",
      "2.271350383758545\n",
      "2.2712483406066895\n",
      "2.271146297454834\n",
      "2.271044969558716\n",
      "step 620: train loss 2.2710, val loss 2.3004\n",
      "2.2709436416625977\n",
      "2.270843267440796\n",
      "2.270742177963257\n",
      "2.270641803741455\n",
      "2.2705419063568115\n",
      "2.270442247390747\n",
      "2.270343065261841\n",
      "2.2702438831329346\n",
      "2.2701449394226074\n",
      "2.2700464725494385\n",
      "step 630: train loss 2.2700, val loss 2.2995\n",
      "2.2699480056762695\n",
      "2.269850015640259\n",
      "2.2697525024414062\n",
      "2.2696549892425537\n",
      "2.2695579528808594\n",
      "2.269460916519165\n",
      "2.269364356994629\n",
      "2.269268274307251\n",
      "2.269171953201294\n",
      "2.269076347351074\n",
      "step 640: train loss 2.2691, val loss 2.2986\n",
      "2.2689807415008545\n",
      "2.268885374069214\n",
      "2.2687907218933105\n",
      "2.268695831298828\n",
      "2.268601417541504\n",
      "2.2685070037841797\n",
      "2.268413543701172\n",
      "2.268319845199585\n",
      "2.268226385116577\n",
      "2.2681331634521484\n",
      "step 650: train loss 2.2681, val loss 2.2978\n",
      "2.268040895462036\n",
      "2.2679481506347656\n",
      "2.267855644226074\n",
      "2.26776385307312\n",
      "2.267671585083008\n",
      "2.267580032348633\n",
      "2.267488956451416\n",
      "2.267397880554199\n",
      "2.2673072814941406\n",
      "2.267216682434082\n",
      "step 660: train loss 2.2672, val loss 2.2970\n",
      "2.2671263217926025\n",
      "2.267036199569702\n",
      "2.266946315765381\n",
      "2.266857147216797\n",
      "2.266767740249634\n",
      "2.266678810119629\n",
      "2.266589879989624\n",
      "2.2665011882781982\n",
      "2.2664129734039307\n",
      "2.266324758529663\n",
      "step 670: train loss 2.2663, val loss 2.2962\n",
      "2.2662370204925537\n",
      "2.2661492824554443\n",
      "2.266062021255493\n",
      "2.265974998474121\n",
      "2.265887975692749\n",
      "2.265801429748535\n",
      "2.2657148838043213\n",
      "2.2656288146972656\n",
      "2.26554274559021\n",
      "2.2654571533203125\n",
      "step 680: train loss 2.2655, val loss 2.2954\n",
      "2.265371799468994\n",
      "2.2652862071990967\n",
      "2.2652013301849365\n",
      "2.2651166915893555\n",
      "2.2650320529937744\n",
      "2.2649476528167725\n",
      "2.2648637294769287\n",
      "2.264779806137085\n",
      "2.2646961212158203\n",
      "2.264612913131714\n",
      "step 690: train loss 2.2646, val loss 2.2947\n",
      "2.264529228210449\n",
      "2.264446258544922\n",
      "2.2643635272979736\n",
      "2.2642810344696045\n",
      "2.2641987800598145\n",
      "2.2641165256500244\n",
      "2.2640345096588135\n",
      "2.2639529705047607\n",
      "2.263871192932129\n",
      "2.2637901306152344\n",
      "step 700: train loss 2.2638, val loss 2.2939\n",
      "2.2637088298797607\n",
      "2.2636282444000244\n",
      "2.263547420501709\n",
      "2.263467311859131\n",
      "2.2633867263793945\n",
      "2.2633068561553955\n",
      "2.2632269859313965\n",
      "2.2631475925445557\n",
      "2.263068199157715\n",
      "2.262989044189453\n",
      "step 710: train loss 2.2630, val loss 2.2932\n",
      "2.2629103660583496\n",
      "2.262830972671509\n",
      "2.2627527713775635\n",
      "2.26267409324646\n",
      "2.262596368789673\n",
      "2.2625181674957275\n",
      "2.2624402046203613\n",
      "2.2623627185821533\n",
      "2.2622852325439453\n",
      "2.2622082233428955\n",
      "step 720: train loss 2.2622, val loss 2.2925\n",
      "2.262131452560425\n",
      "2.262054681777954\n",
      "2.2619779109954834\n",
      "2.261901617050171\n",
      "2.2618255615234375\n",
      "2.261749505996704\n",
      "2.26167368888855\n",
      "2.2615978717803955\n",
      "2.2615225315093994\n",
      "2.2614474296569824\n",
      "step 730: train loss 2.2614, val loss 2.2919\n",
      "2.2613723278045654\n",
      "2.2612974643707275\n",
      "2.2612226009368896\n",
      "2.26114821434021\n",
      "2.2610743045806885\n",
      "2.2609996795654297\n",
      "2.260925769805908\n",
      "2.260852098464966\n",
      "2.2607786655426025\n",
      "2.2607052326202393\n",
      "step 740: train loss 2.2607, val loss 2.2912\n",
      "2.260632276535034\n",
      "2.26055908203125\n",
      "2.260486364364624\n",
      "2.260413646697998\n",
      "2.260341167449951\n",
      "2.2602689266204834\n",
      "2.2601969242095947\n",
      "2.260124683380127\n",
      "2.2600529193878174\n",
      "2.259981393814087\n",
      "step 750: train loss 2.2600, val loss 2.2906\n",
      "2.2599101066589355\n",
      "2.259838819503784\n",
      "2.259767770767212\n",
      "2.2596969604492188\n",
      "2.259626626968384\n",
      "2.2595555782318115\n",
      "2.2594854831695557\n",
      "2.2594151496887207\n",
      "2.259345054626465\n",
      "2.259275436401367\n",
      "step 760: train loss 2.2593, val loss 2.2900\n",
      "2.2592060565948486\n",
      "2.259136199951172\n",
      "2.2590668201446533\n",
      "2.258997678756714\n",
      "2.2589285373687744\n",
      "2.2588601112365723\n",
      "2.258791208267212\n",
      "2.2587227821350098\n",
      "2.2586543560028076\n",
      "2.2585861682891846\n",
      "step 770: train loss 2.2586, val loss 2.2894\n",
      "2.2585179805755615\n",
      "2.2584502696990967\n",
      "2.258382797241211\n",
      "2.258314847946167\n",
      "2.2582478523254395\n",
      "2.2581803798675537\n",
      "2.258113384246826\n",
      "2.258046865463257\n",
      "2.2579798698425293\n",
      "2.25791335105896\n",
      "step 780: train loss 2.2579, val loss 2.2888\n",
      "2.2578468322753906\n",
      "2.2577803134918213\n",
      "2.2577145099639893\n",
      "2.2576487064361572\n",
      "2.257582902908325\n",
      "2.257517099380493\n",
      "2.2574515342712402\n",
      "2.2573866844177246\n",
      "2.2573211193084717\n",
      "2.257256507873535\n",
      "step 790: train loss 2.2573, val loss 2.2882\n",
      "2.2571916580200195\n",
      "2.257126808166504\n",
      "2.2570624351501465\n",
      "2.25699782371521\n",
      "2.2569336891174316\n",
      "2.256869316101074\n",
      "2.256805658340454\n",
      "2.256741762161255\n",
      "2.2566781044006348\n",
      "Reg strength: 5.000, Train loss: 2.2567, Val loss: 2.2882\n",
      "\n",
      "Best regularization strength: 0.0\n",
      "3.748319149017334\n",
      "step 0: train loss 3.7483, val loss 3.7543\n",
      "3.6344263553619385\n",
      "3.550145387649536\n",
      "3.484727382659912\n",
      "3.429079294204712\n",
      "3.3790476322174072\n",
      "3.3334155082702637\n",
      "3.2915987968444824\n",
      "3.253203868865967\n",
      "3.2179064750671387\n",
      "3.185405731201172\n",
      "step 10: train loss 3.1854, val loss 3.1927\n",
      "3.155413866043091\n",
      "3.1276533603668213\n",
      "3.1018667221069336\n",
      "3.0778205394744873\n",
      "3.0553109645843506\n",
      "3.034165382385254\n",
      "3.014239549636841\n",
      "2.9954140186309814\n",
      "2.9775891304016113\n",
      "2.960679054260254\n",
      "step 20: train loss 2.9607, val loss 2.9687\n",
      "2.944612503051758\n",
      "2.929326057434082\n",
      "2.9147629737854004\n",
      "2.9008727073669434\n",
      "2.8876099586486816\n",
      "2.8749325275421143\n",
      "2.862802743911743\n",
      "2.851184844970703\n",
      "2.8400468826293945\n",
      "2.8293585777282715\n",
      "step 30: train loss 2.8294, val loss 2.8381\n",
      "2.819092273712158\n",
      "2.8092219829559326\n",
      "2.7997238636016846\n",
      "2.7905750274658203\n",
      "2.781755208969116\n",
      "2.7732458114624023\n",
      "2.7650275230407715\n",
      "2.757084846496582\n",
      "2.7494020462036133\n",
      "2.741964340209961\n",
      "step 40: train loss 2.7420, val loss 2.7521\n",
      "2.7347593307495117\n",
      "2.7277746200561523\n",
      "2.7209982872009277\n",
      "2.7144196033477783\n",
      "2.708029270172119\n",
      "2.7018179893493652\n",
      "2.6957767009735107\n",
      "2.6898982524871826\n",
      "2.6841742992401123\n",
      "2.678598642349243\n",
      "step 50: train loss 2.6786, val loss 2.6902\n",
      "2.6731646060943604\n",
      "2.6678664684295654\n",
      "2.6626980304718018\n",
      "2.65765380859375\n",
      "2.6527297496795654\n",
      "2.6479198932647705\n",
      "2.643221139907837\n",
      "2.6386282444000244\n",
      "2.6341378688812256\n",
      "2.6297459602355957\n",
      "step 60: train loss 2.6297, val loss 2.6427\n",
      "2.6254496574401855\n",
      "2.6212451457977295\n",
      "2.61712908744812\n",
      "2.6130993366241455\n",
      "2.609152317047119\n",
      "2.605285882949829\n",
      "2.601496934890747\n",
      "2.597783327102661\n",
      "2.5941429138183594\n",
      "2.5905730724334717\n",
      "step 70: train loss 2.5906, val loss 2.6046\n",
      "2.5870726108551025\n",
      "2.5836384296417236\n",
      "2.5802688598632812\n",
      "2.576962471008301\n",
      "2.5737175941467285\n",
      "2.5705316066741943\n",
      "2.567403554916382\n",
      "2.5643320083618164\n",
      "2.5613152980804443\n",
      "2.5583510398864746\n",
      "step 80: train loss 2.5584, val loss 2.5733\n",
      "2.5554392337799072\n",
      "2.5525779724121094\n",
      "2.5497658252716064\n",
      "2.547001361846924\n",
      "2.544283390045166\n",
      "2.541611433029175\n",
      "2.5389838218688965\n",
      "2.5363991260528564\n",
      "2.5338566303253174\n",
      "2.5313549041748047\n",
      "step 90: train loss 2.5314, val loss 2.5471\n",
      "2.52889347076416\n",
      "2.526470899581909\n",
      "2.5240867137908936\n",
      "2.5217397212982178\n",
      "2.5194287300109863\n",
      "2.517153263092041\n",
      "2.5149123668670654\n",
      "2.5127053260803223\n",
      "2.510530948638916\n",
      "2.5083887577056885\n",
      "step 100: train loss 2.5084, val loss 2.5249\n",
      "2.5062780380249023\n",
      "2.5041980743408203\n",
      "2.502147912979126\n",
      "2.5001273155212402\n",
      "2.4981348514556885\n",
      "2.49617075920105\n",
      "2.4942336082458496\n",
      "2.492323637008667\n",
      "2.4904396533966064\n",
      "2.4885809421539307\n",
      "step 110: train loss 2.4886, val loss 2.5057\n",
      "2.4867477416992188\n",
      "2.484938621520996\n",
      "2.483153820037842\n",
      "2.4813923835754395\n",
      "2.4796535968780518\n",
      "2.477937698364258\n",
      "2.476243019104004\n",
      "2.4745705127716064\n",
      "2.472919225692749\n",
      "2.4712882041931152\n",
      "step 120: train loss 2.4713, val loss 2.4891\n",
      "2.469677448272705\n",
      "2.4680871963500977\n",
      "2.46651554107666\n",
      "2.464963674545288\n",
      "2.4634299278259277\n",
      "2.4619152545928955\n",
      "2.460418462753296\n",
      "2.4589388370513916\n",
      "2.45747709274292\n",
      "2.4560320377349854\n",
      "step 130: train loss 2.4560, val loss 2.4744\n",
      "2.454603910446167\n",
      "2.4531919956207275\n",
      "2.451796531677246\n",
      "2.4504165649414062\n",
      "2.449052572250366\n",
      "2.4477038383483887\n",
      "2.4463696479797363\n",
      "2.4450504779815674\n",
      "2.4437460899353027\n",
      "2.442455768585205\n",
      "step 140: train loss 2.4425, val loss 2.4613\n",
      "2.441179037094116\n",
      "2.4399168491363525\n",
      "2.4386675357818604\n",
      "2.437432050704956\n",
      "2.4362096786499023\n",
      "2.434999942779541\n",
      "2.433803081512451\n",
      "2.4326186180114746\n",
      "2.4314465522766113\n",
      "2.430286407470703\n",
      "step 150: train loss 2.4303, val loss 2.4497\n",
      "2.429138660430908\n",
      "2.42800235748291\n",
      "2.42687726020813\n",
      "2.4257640838623047\n",
      "2.424661636352539\n",
      "2.4235706329345703\n",
      "2.422490119934082\n",
      "2.4214208126068115\n",
      "2.4203615188598633\n",
      "2.4193129539489746\n",
      "step 160: train loss 2.4193, val loss 2.4392\n",
      "2.4182746410369873\n",
      "2.417246103286743\n",
      "2.4162278175354004\n",
      "2.415219306945801\n",
      "2.4142203330993652\n",
      "2.413231134414673\n",
      "2.4122509956359863\n",
      "2.411280393600464\n",
      "2.4103188514709473\n",
      "2.4093658924102783\n",
      "step 170: train loss 2.4094, val loss 2.4297\n",
      "2.4084219932556152\n",
      "2.407486915588379\n",
      "2.4065608978271484\n",
      "2.4056429862976074\n",
      "2.404733180999756\n",
      "2.403832197189331\n",
      "2.4029390811920166\n",
      "2.4020540714263916\n",
      "2.401176929473877\n",
      "2.4003076553344727\n",
      "step 180: train loss 2.4003, val loss 2.4211\n",
      "2.3994462490081787\n",
      "2.398592233657837\n",
      "2.3977463245391846\n",
      "2.396907329559326\n",
      "2.39607572555542\n",
      "2.395251750946045\n",
      "2.394434690475464\n",
      "2.3936245441436768\n",
      "2.3928213119506836\n",
      "2.3920249938964844\n",
      "step 190: train loss 2.3920, val loss 2.4132\n",
      "2.391235589981079\n",
      "2.3904528617858887\n",
      "2.389677047729492\n",
      "2.3889071941375732\n",
      "2.388144016265869\n",
      "2.38738751411438\n",
      "2.3866372108459473\n",
      "2.385892629623413\n",
      "2.3851547241210938\n",
      "2.384422540664673\n",
      "step 200: train loss 2.3844, val loss 2.4060\n",
      "2.3836965560913086\n",
      "2.382976531982422\n",
      "2.3822622299194336\n",
      "2.3815536499023438\n",
      "2.3808507919311523\n",
      "2.3801538944244385\n",
      "2.379462242126465\n",
      "2.3787760734558105\n",
      "2.3780951499938965\n",
      "2.37742018699646\n",
      "step 210: train loss 2.3774, val loss 2.3993\n",
      "2.3767499923706055\n",
      "2.3760855197906494\n",
      "2.3754255771636963\n",
      "2.3747713565826416\n",
      "2.374121904373169\n",
      "2.3734774589538574\n",
      "2.372838020324707\n",
      "2.3722035884857178\n",
      "2.3715741634368896\n",
      "2.3709487915039062\n",
      "step 220: train loss 2.3709, val loss 2.3932\n",
      "2.370328903198242\n",
      "2.369713306427002\n",
      "2.3691024780273438\n",
      "2.3684959411621094\n",
      "2.367893934249878\n",
      "2.3672964572906494\n",
      "2.366703748703003\n",
      "2.366114616394043\n",
      "2.365530490875244\n",
      "2.364950180053711\n",
      "step 230: train loss 2.3650, val loss 2.3875\n",
      "2.3643743991851807\n",
      "2.363802671432495\n",
      "2.3632349967956543\n",
      "2.3626718521118164\n",
      "2.362112045288086\n",
      "2.3615562915802\n",
      "2.3610050678253174\n",
      "2.360457181930542\n",
      "2.3599138259887695\n",
      "2.3593735694885254\n",
      "step 240: train loss 2.3594, val loss 2.3823\n",
      "2.358837604522705\n",
      "2.358304977416992\n",
      "2.357776403427124\n",
      "2.3572511672973633\n",
      "2.356729507446289\n",
      "2.3562116622924805\n",
      "2.3556973934173584\n",
      "2.3551864624023438\n",
      "2.3546793460845947\n",
      "2.354175090789795\n",
      "step 250: train loss 2.3542, val loss 2.3774\n",
      "2.3536744117736816\n",
      "2.353177547454834\n",
      "2.3526835441589355\n",
      "2.3521928787231445\n",
      "2.351705551147461\n",
      "2.3512213230133057\n",
      "2.350740671157837\n",
      "2.3502628803253174\n",
      "2.349788188934326\n",
      "2.3493165969848633\n",
      "step 260: train loss 2.3493, val loss 2.3728\n",
      "2.348848342895508\n",
      "2.3483827114105225\n",
      "2.3479204177856445\n",
      "2.347460985183716\n",
      "2.3470048904418945\n",
      "2.346550703048706\n",
      "2.3461005687713623\n",
      "2.3456525802612305\n",
      "2.345207452774048\n",
      "2.3447651863098145\n",
      "step 270: train loss 2.3448, val loss 2.3685\n",
      "2.3443257808685303\n",
      "2.3438894748687744\n",
      "2.3434555530548096\n",
      "2.3430237770080566\n",
      "2.3425955772399902\n",
      "2.3421695232391357\n",
      "2.3417465686798096\n",
      "2.3413257598876953\n",
      "2.340907573699951\n",
      "2.3404922485351562\n",
      "step 280: train loss 2.3405, val loss 2.3645\n",
      "2.3400790691375732\n",
      "2.3396685123443604\n",
      "2.3392605781555176\n",
      "2.338855028152466\n",
      "2.338451862335205\n",
      "2.3380510807037354\n",
      "2.3376526832580566\n",
      "2.337257146835327\n",
      "2.3368632793426514\n",
      "2.3364717960357666\n",
      "step 290: train loss 2.3365, val loss 2.3607\n",
      "2.336082935333252\n",
      "2.33569598197937\n",
      "2.3353116512298584\n",
      "2.3349294662475586\n",
      "2.334549903869629\n",
      "2.334171772003174\n",
      "2.3337960243225098\n",
      "2.3334226608276367\n",
      "2.3330512046813965\n",
      "2.3326823711395264\n",
      "step 300: train loss 2.3327, val loss 2.3572\n",
      "2.332314968109131\n",
      "2.3319504261016846\n",
      "2.331587314605713\n",
      "2.3312265872955322\n",
      "2.3308680057525635\n",
      "2.3305110931396484\n",
      "2.3301565647125244\n",
      "2.329803466796875\n",
      "2.3294527530670166\n",
      "2.329103946685791\n",
      "step 310: train loss 2.3291, val loss 2.3538\n",
      "2.3287570476531982\n",
      "2.3284120559692383\n",
      "2.328068733215332\n",
      "2.327727794647217\n",
      "2.3273885250091553\n",
      "2.3270506858825684\n",
      "2.3267149925231934\n",
      "2.3263814449310303\n",
      "2.326049327850342\n",
      "2.325718879699707\n",
      "step 320: train loss 2.3257, val loss 2.3507\n",
      "2.3253908157348633\n",
      "2.325063943862915\n",
      "2.3247389793395996\n",
      "2.324415922164917\n",
      "2.324094295501709\n",
      "2.323774814605713\n",
      "2.3234565258026123\n",
      "2.3231403827667236\n",
      "2.3228254318237305\n",
      "2.322512626647949\n",
      "step 330: train loss 2.3225, val loss 2.3477\n",
      "2.3222012519836426\n",
      "2.3218917846679688\n",
      "2.3215832710266113\n",
      "2.321276903152466\n",
      "2.320971727371216\n",
      "2.3206684589385986\n",
      "2.320366859436035\n",
      "2.320066452026367\n",
      "2.319767475128174\n",
      "2.3194706439971924\n",
      "step 340: train loss 2.3195, val loss 2.3449\n",
      "2.3191750049591064\n",
      "2.318881034851074\n",
      "2.3185882568359375\n",
      "2.3182971477508545\n",
      "2.318007230758667\n",
      "2.317718744277954\n",
      "2.317432403564453\n",
      "2.3171470165252686\n",
      "2.3168630599975586\n",
      "2.3165805339813232\n",
      "step 350: train loss 2.3166, val loss 2.3422\n",
      "2.3162994384765625\n",
      "2.3160197734832764\n",
      "2.3157413005828857\n",
      "2.315464496612549\n",
      "2.3151888847351074\n",
      "2.3149147033691406\n",
      "2.3146421909332275\n",
      "2.3143701553344727\n",
      "2.314100503921509\n",
      "2.313831090927124\n",
      "step 360: train loss 2.3138, val loss 2.3396\n",
      "2.313563823699951\n",
      "2.3132975101470947\n",
      "2.313032627105713\n",
      "2.3127686977386475\n",
      "2.3125061988830566\n",
      "2.3122448921203613\n",
      "2.311985492706299\n",
      "2.3117268085479736\n",
      "2.311469078063965\n",
      "2.3112130165100098\n",
      "step 370: train loss 2.3112, val loss 2.3372\n",
      "2.310957670211792\n",
      "2.310703992843628\n",
      "2.3104512691497803\n",
      "2.3101999759674072\n",
      "2.3099498748779297\n",
      "2.3097007274627686\n",
      "2.309452533721924\n",
      "2.309206008911133\n",
      "2.308960199356079\n",
      "2.3087158203125\n",
      "step 380: train loss 2.3087, val loss 2.3348\n",
      "2.3084723949432373\n",
      "2.308230400085449\n",
      "2.3079893589019775\n",
      "2.3077495098114014\n",
      "2.3075106143951416\n",
      "2.3072726726531982\n",
      "2.3070361614227295\n",
      "2.306800603866577\n",
      "2.306565761566162\n",
      "2.3063321113586426\n",
      "step 390: train loss 2.3063, val loss 2.3326\n",
      "2.3060998916625977\n",
      "2.30586838722229\n",
      "2.305638074874878\n",
      "2.3054089546203613\n",
      "2.305180549621582\n",
      "2.3049533367156982\n",
      "2.304727077484131\n",
      "2.30450177192688\n",
      "2.3042776584625244\n",
      "2.3040544986724854\n",
      "step 400: train loss 2.3041, val loss 2.3305\n",
      "2.3038320541381836\n",
      "2.3036108016967773\n",
      "2.3033907413482666\n",
      "2.303171396255493\n",
      "2.302952527999878\n",
      "2.3027353286743164\n",
      "2.3025190830230713\n",
      "2.3023033142089844\n",
      "2.302088975906372\n",
      "2.301875114440918\n",
      "step 410: train loss 2.3019, val loss 2.3285\n",
      "2.3016622066497803\n",
      "2.301450490951538\n",
      "2.3012397289276123\n",
      "2.301029682159424\n",
      "2.300820827484131\n",
      "2.300612688064575\n",
      "2.300405263900757\n",
      "2.300198793411255\n",
      "2.2999930381774902\n",
      "2.299788475036621\n",
      "step 420: train loss 2.2998, val loss 2.3266\n",
      "2.2995846271514893\n",
      "2.2993812561035156\n",
      "2.2991795539855957\n",
      "2.298978328704834\n",
      "2.2987778186798096\n",
      "2.2985782623291016\n",
      "2.298379421234131\n",
      "2.2981815338134766\n",
      "2.2979843616485596\n",
      "2.297788143157959\n",
      "step 430: train loss 2.2978, val loss 2.3247\n",
      "2.2975926399230957\n",
      "2.2973978519439697\n",
      "2.2972044944763184\n",
      "2.297011137008667\n",
      "2.296818733215332\n",
      "2.2966272830963135\n",
      "2.2964365482330322\n",
      "2.2962465286254883\n",
      "2.2960574626922607\n",
      "2.2958691120147705\n",
      "step 440: train loss 2.2959, val loss 2.3230\n",
      "2.2956814765930176\n",
      "2.295494556427002\n",
      "2.2953083515167236\n",
      "2.2951231002807617\n",
      "2.294938325881958\n",
      "2.29475474357605\n",
      "2.2945716381073\n",
      "2.294389009475708\n",
      "2.2942073345184326\n",
      "2.2940263748168945\n",
      "step 450: train loss 2.2940, val loss 2.3213\n",
      "2.293846368789673\n",
      "2.2936668395996094\n",
      "2.293487787246704\n",
      "2.2933096885681152\n",
      "2.2931320667266846\n",
      "2.2929553985595703\n",
      "2.2927796840667725\n",
      "2.292604446411133\n",
      "2.2924294471740723\n",
      "2.2922556400299072\n",
      "step 460: train loss 2.2923, val loss 2.3196\n",
      "2.2920823097229004\n",
      "2.2919094562530518\n",
      "2.2917380332946777\n",
      "2.2915663719177246\n",
      "2.291395664215088\n",
      "2.2912256717681885\n",
      "2.2910563945770264\n",
      "2.2908878326416016\n",
      "2.290719747543335\n",
      "2.2905526161193848\n",
      "step 470: train loss 2.2906, val loss 2.3181\n",
      "2.2903854846954346\n",
      "2.29021954536438\n",
      "2.290053606033325\n",
      "2.289888858795166\n",
      "2.289724588394165\n",
      "2.2895610332489014\n",
      "2.289398193359375\n",
      "2.2892355918884277\n",
      "2.289073944091797\n",
      "2.288912534713745\n",
      "step 480: train loss 2.2889, val loss 2.3166\n",
      "2.2887520790100098\n",
      "2.2885921001434326\n",
      "2.2884328365325928\n",
      "2.288274049758911\n",
      "2.2881157398223877\n",
      "2.2879581451416016\n",
      "2.2878010272979736\n",
      "2.287644624710083\n",
      "2.2874886989593506\n",
      "2.2873332500457764\n",
      "step 490: train loss 2.2873, val loss 2.3151\n",
      "2.2871785163879395\n",
      "2.2870242595672607\n",
      "2.2868707180023193\n",
      "2.2867178916931152\n",
      "2.2865653038024902\n",
      "2.2864134311676025\n",
      "2.286261796951294\n",
      "2.2861108779907227\n",
      "2.2859609127044678\n",
      "2.285810947418213\n",
      "step 500: train loss 2.2858, val loss 2.3137\n",
      "2.2856616973876953\n",
      "2.285512685775757\n",
      "2.285364866256714\n",
      "2.285217046737671\n",
      "2.2850699424743652\n",
      "2.2849233150482178\n",
      "2.2847771644592285\n",
      "2.2846317291259766\n",
      "2.2844865322113037\n",
      "2.284342050552368\n",
      "step 510: train loss 2.2843, val loss 2.3124\n",
      "2.284198045730591\n",
      "2.2840545177459717\n",
      "2.28391170501709\n",
      "2.283769130706787\n",
      "2.2836267948150635\n",
      "2.2834854125976562\n",
      "2.283344268798828\n",
      "2.2832038402557373\n",
      "2.2830638885498047\n",
      "2.2829244136810303\n",
      "step 520: train loss 2.2829, val loss 2.3111\n",
      "2.282785177230835\n",
      "2.282646656036377\n",
      "2.282508611679077\n",
      "2.2823708057403564\n",
      "2.282233476638794\n",
      "2.2820968627929688\n",
      "2.2819604873657227\n",
      "2.281824827194214\n",
      "2.2816896438598633\n",
      "2.281554698944092\n",
      "step 530: train loss 2.2816, val loss 2.3099\n",
      "2.2814204692840576\n",
      "2.2812862396240234\n",
      "2.2811529636383057\n",
      "2.281020164489746\n",
      "2.2808876037597656\n",
      "2.280755043029785\n",
      "2.280623435974121\n",
      "2.2804923057556152\n",
      "2.2803616523742676\n",
      "2.280231237411499\n",
      "step 540: train loss 2.2802, val loss 2.3087\n",
      "2.2801010608673096\n",
      "2.2799715995788574\n",
      "2.2798426151275635\n",
      "2.2797138690948486\n",
      "2.279585361480713\n",
      "2.2794580459594727\n",
      "2.2793307304382324\n",
      "2.2792036533355713\n",
      "2.2790772914886475\n",
      "2.2789509296417236\n",
      "step 550: train loss 2.2790, val loss 2.3075\n",
      "2.278825283050537\n",
      "2.278700113296509\n",
      "2.2785754203796387\n",
      "2.2784507274627686\n",
      "2.2783267498016357\n",
      "2.278202772140503\n",
      "2.2780797481536865\n",
      "2.277956962585449\n",
      "2.277834415435791\n",
      "2.277712345123291\n",
      "step 560: train loss 2.2777, val loss 2.3064\n",
      "2.27759051322937\n",
      "2.2774691581726074\n",
      "2.277348518371582\n",
      "2.2772278785705566\n",
      "2.2771077156066895\n",
      "2.2769882678985596\n",
      "2.2768685817718506\n",
      "2.276749610900879\n",
      "2.2766308784484863\n",
      "2.27651309967041\n",
      "step 570: train loss 2.2765, val loss 2.3053\n",
      "2.276395082473755\n",
      "2.2762773036956787\n",
      "2.27616024017334\n",
      "2.2760441303253174\n",
      "2.2759275436401367\n",
      "2.2758114337921143\n",
      "2.275695562362671\n",
      "2.2755801677703857\n",
      "2.275465488433838\n",
      "2.275351047515869\n",
      "step 580: train loss 2.2754, val loss 2.3042\n",
      "2.2752370834350586\n",
      "2.275123119354248\n",
      "2.2750096321105957\n",
      "2.2748966217041016\n",
      "2.2747838497161865\n",
      "2.2746713161468506\n",
      "2.2745590209960938\n",
      "2.274447202682495\n",
      "2.2743358612060547\n",
      "2.2742249965667725\n",
      "step 590: train loss 2.2742, val loss 2.3032\n",
      "2.2741141319274902\n",
      "2.274003744125366\n",
      "2.2738935947418213\n",
      "2.2737841606140137\n",
      "2.273674726486206\n",
      "2.2735657691955566\n",
      "2.2734570503234863\n",
      "2.273348569869995\n",
      "2.273240566253662\n",
      "2.273132801055908\n",
      "step 600: train loss 2.2731, val loss 2.3022\n",
      "2.2730252742767334\n",
      "2.272918462753296\n",
      "2.2728116512298584\n",
      "2.272705316543579\n",
      "2.272599220275879\n",
      "2.272493362426758\n",
      "2.272387981414795\n",
      "2.272282600402832\n",
      "2.2721779346466064\n",
      "2.27207350730896\n",
      "step 610: train loss 2.2721, val loss 2.3013\n",
      "2.2719690799713135\n",
      "2.271865129470825\n",
      "2.271761655807495\n",
      "2.271658182144165\n",
      "2.271555185317993\n",
      "2.2714526653289795\n",
      "2.271350383758545\n",
      "2.2712483406066895\n",
      "2.271146297454834\n",
      "2.271044969558716\n",
      "step 620: train loss 2.2710, val loss 2.3004\n",
      "2.2709436416625977\n",
      "2.270843267440796\n",
      "2.270742177963257\n",
      "2.270641803741455\n",
      "2.2705419063568115\n",
      "2.270442247390747\n",
      "2.270343065261841\n",
      "2.2702438831329346\n",
      "2.2701449394226074\n",
      "2.2700464725494385\n",
      "step 630: train loss 2.2700, val loss 2.2995\n",
      "2.2699480056762695\n",
      "2.269850015640259\n",
      "2.2697525024414062\n",
      "2.2696549892425537\n",
      "2.2695579528808594\n",
      "2.269460916519165\n",
      "2.269364356994629\n",
      "2.269268274307251\n",
      "2.269171953201294\n",
      "2.269076347351074\n",
      "step 640: train loss 2.2691, val loss 2.2986\n",
      "2.2689807415008545\n",
      "2.268885374069214\n",
      "2.2687907218933105\n",
      "2.268695831298828\n",
      "2.268601417541504\n",
      "2.2685070037841797\n",
      "2.268413543701172\n",
      "2.268319845199585\n",
      "2.268226385116577\n",
      "2.2681331634521484\n",
      "step 650: train loss 2.2681, val loss 2.2978\n",
      "2.268040895462036\n",
      "2.2679481506347656\n",
      "2.267855644226074\n",
      "2.26776385307312\n",
      "2.267671585083008\n",
      "2.267580032348633\n",
      "2.267488956451416\n",
      "2.267397880554199\n",
      "2.2673072814941406\n",
      "2.267216682434082\n",
      "step 660: train loss 2.2672, val loss 2.2970\n",
      "2.2671263217926025\n",
      "2.267036199569702\n",
      "2.266946315765381\n",
      "2.266857147216797\n",
      "2.266767740249634\n",
      "2.266678810119629\n",
      "2.266589879989624\n",
      "2.2665011882781982\n",
      "2.2664129734039307\n",
      "2.266324758529663\n",
      "step 670: train loss 2.2663, val loss 2.2962\n",
      "2.2662370204925537\n",
      "2.2661492824554443\n",
      "2.266062021255493\n",
      "2.265974998474121\n",
      "2.265887975692749\n",
      "2.265801429748535\n",
      "2.2657148838043213\n",
      "2.2656288146972656\n",
      "2.26554274559021\n",
      "2.2654571533203125\n",
      "step 680: train loss 2.2655, val loss 2.2954\n",
      "2.265371799468994\n",
      "2.2652862071990967\n",
      "2.2652013301849365\n",
      "2.2651166915893555\n",
      "2.2650320529937744\n",
      "2.2649476528167725\n",
      "2.2648637294769287\n",
      "2.264779806137085\n",
      "2.2646961212158203\n",
      "2.264612913131714\n",
      "step 690: train loss 2.2646, val loss 2.2947\n",
      "2.264529228210449\n",
      "2.264446258544922\n",
      "2.2643635272979736\n",
      "2.2642810344696045\n",
      "2.2641987800598145\n",
      "2.2641165256500244\n",
      "2.2640345096588135\n",
      "2.2639529705047607\n",
      "2.263871192932129\n",
      "2.2637901306152344\n",
      "step 700: train loss 2.2638, val loss 2.2939\n",
      "2.2637088298797607\n",
      "2.2636282444000244\n",
      "2.263547420501709\n",
      "2.263467311859131\n",
      "2.2633867263793945\n",
      "2.2633068561553955\n",
      "2.2632269859313965\n",
      "2.2631475925445557\n",
      "2.263068199157715\n",
      "2.262989044189453\n",
      "step 710: train loss 2.2630, val loss 2.2932\n",
      "2.2629103660583496\n",
      "2.262830972671509\n",
      "2.2627527713775635\n",
      "2.26267409324646\n",
      "2.262596368789673\n",
      "2.2625181674957275\n",
      "2.2624402046203613\n",
      "2.2623627185821533\n",
      "2.2622852325439453\n",
      "2.2622082233428955\n",
      "step 720: train loss 2.2622, val loss 2.2925\n",
      "2.262131452560425\n",
      "2.262054681777954\n",
      "2.2619779109954834\n",
      "2.261901617050171\n",
      "2.2618255615234375\n",
      "2.261749505996704\n",
      "2.26167368888855\n",
      "2.2615978717803955\n",
      "2.2615225315093994\n",
      "2.2614474296569824\n",
      "step 730: train loss 2.2614, val loss 2.2919\n",
      "2.2613723278045654\n",
      "2.2612974643707275\n",
      "2.2612226009368896\n",
      "2.26114821434021\n",
      "2.2610743045806885\n",
      "2.2609996795654297\n",
      "2.260925769805908\n",
      "2.260852098464966\n",
      "2.2607786655426025\n",
      "2.2607052326202393\n",
      "step 740: train loss 2.2607, val loss 2.2912\n",
      "2.260632276535034\n",
      "2.26055908203125\n",
      "2.260486364364624\n",
      "2.260413646697998\n",
      "2.260341167449951\n",
      "2.2602689266204834\n",
      "2.2601969242095947\n",
      "2.260124683380127\n",
      "2.2600529193878174\n",
      "2.259981393814087\n",
      "step 750: train loss 2.2600, val loss 2.2906\n",
      "2.2599101066589355\n",
      "2.259838819503784\n",
      "2.259767770767212\n",
      "2.2596969604492188\n",
      "2.259626626968384\n",
      "2.2595555782318115\n",
      "2.2594854831695557\n",
      "2.2594151496887207\n",
      "2.259345054626465\n",
      "2.259275436401367\n",
      "step 760: train loss 2.2593, val loss 2.2900\n",
      "2.2592060565948486\n",
      "2.259136199951172\n",
      "2.2590668201446533\n",
      "2.258997678756714\n",
      "2.2589285373687744\n",
      "2.2588601112365723\n",
      "2.258791208267212\n",
      "2.2587227821350098\n",
      "2.2586543560028076\n",
      "2.2585861682891846\n",
      "step 770: train loss 2.2586, val loss 2.2894\n",
      "2.2585179805755615\n",
      "2.2584502696990967\n",
      "2.258382797241211\n",
      "2.258314847946167\n",
      "2.2582478523254395\n",
      "2.2581803798675537\n",
      "2.258113384246826\n",
      "2.258046865463257\n",
      "2.2579798698425293\n",
      "2.25791335105896\n",
      "step 780: train loss 2.2579, val loss 2.2888\n",
      "2.2578468322753906\n"
     ]
    }
   ],
   "source": [
    "def train_model(reg_strength, num_epochs=800):\n",
    "    g = torch.Generator().manual_seed(1478)\n",
    "    W = torch.randn((27*27, 27), generator=g, requires_grad=True)\n",
    "    \n",
    "    # Lists to store losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # gradient descent\n",
    "    for k in range(num_epochs):\n",
    "        # forward pass\n",
    "        xenc = F.one_hot(xs, num_classes=27*27).float() # input to the network: one-hot encoding\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "        loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()\n",
    "        print(loss.item())\n",
    "    \n",
    "        # Store training loss\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Every 10 epochs, calculate validation loss\n",
    "        if k % 10 == 0:\n",
    "            # Calculate validation loss\n",
    "            with torch.no_grad():\n",
    "                val_xenc = F.one_hot(val_xs, num_classes=27*27).float()\n",
    "                val_logits = val_xenc @ W\n",
    "                val_counts = val_logits.exp()\n",
    "                val_probs = val_counts / val_counts.sum(1, keepdims=True)\n",
    "                val_loss = -val_probs[torch.arange(len(val_ys)), val_ys].log().mean() + 0.01*(W**2).mean()\n",
    "                print(f'step {k}: train loss {loss.item():.4f}, val loss {val_loss.item():.4f}')\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "        # backward pass\n",
    "        W.grad = None # set to zero the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        W.data += -50 * W.grad\n",
    "        \n",
    "    return train_losses[-1], val_losses[-1], W\n",
    "\n",
    "reg_strengths = [0.0, 0.001, 0.1, 0.5, 5.0]\n",
    "results = []\n",
    "\n",
    "for reg in reg_strengths:\n",
    "    train_loss, val_loss, W = train_model(reg)\n",
    "    results.append({\n",
    "        'reg_strength': reg,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    })\n",
    "    print(f'Reg strength: {reg:.3f}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}')\n",
    "\n",
    "# Find best regularization strength based on validation loss\n",
    "best_reg = min(results, key=lambda x: x['val_loss'])['reg_strength']\n",
    "print(f'\\nBest regularization strength: {best_reg}')\n",
    "\n",
    "# Train final model with best regularization \n",
    "final_train_loss, final_val_loss, best_W = train_model(best_reg)\n",
    "\n",
    "# Evaluate on test set\n",
    "with torch.no_grad():\n",
    "    test_xenc = F.one_hot(test_xs, num_classes=27*27).float()\n",
    "    test_logits = test_xenc @ best_W\n",
    "    test_counts = test_logits.exp()\n",
    "    test_probs = test_counts / test_counts.sum(1, keepdims=True)\n",
    "    test_loss = -test_probs[torch.arange(len(test_ys)), test_ys].log().mean() + best_reg*(best_W**2).mean()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predictions = test_probs.argmax(1)\n",
    "    accuracy = (predictions == test_ys).float().mean()\n",
    "    \n",
    "print(f'\\nFinal Results with best reg strength {best_reg}:')\n",
    "print(f'Test loss: {test_loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.tanasid.prelay.adin.fai.ritonian.free.udiania.zabileniassdbduinewimbressiyanayla.perinviumtsy\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "num_chars=100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "out = ['.', '.']  # start with two dots\n",
    "for _ in range(num_chars):\n",
    "    # Get the last two characters\n",
    "    ch1, ch2 = out[-2], out[-1]\n",
    "    ix1, ix2 = stoi[ch1], stoi[ch2]\n",
    "    \n",
    "    # Create combined index\n",
    "    combined_ix = ix1 * 27 + ix2\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    xenc = F.one_hot(torch.tensor([combined_ix]), num_classes=27*27).float()\n",
    "    \n",
    "    # Get probabilities\n",
    "    logits = xenc @ best_W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    \n",
    "    # Sample from the distribution\n",
    "    ix = torch.multinomial(probs[0], num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos[ix])\n",
    "print(''.join(out[2:]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
